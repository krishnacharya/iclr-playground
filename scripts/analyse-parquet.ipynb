{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38934507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>year</th>\n",
       "      <th>openreview_link</th>\n",
       "      <th>pdf_download_link</th>\n",
       "      <th>title</th>\n",
       "      <th>original_abstract</th>\n",
       "      <th>original_reviews</th>\n",
       "      <th>normalized_reviews</th>\n",
       "      <th>original_metareview</th>\n",
       "      <th>technical_indicators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ryxz8CVYDH</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=ryxz8CVYDH</td>\n",
       "      <td>https://openreview.net/pdf/07a4b4b413b37f2c43c...</td>\n",
       "      <td>Learning to Learn by Zeroth-Order Oracle</td>\n",
       "      <td>In the learning to learn (L2L) framework, we c...</td>\n",
       "      <td>[{\"rating\": \"6: Weak Accept\", \"review\": \"The p...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper introduces a zeroth...</td>\n",
       "      <td>{\"decision\": \"Accept (Poster)\", \"comment\": \"Th...</td>\n",
       "      <td>{\"binary_decision\": \"accept\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ryxyCeHtPB</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=ryxyCeHtPB</td>\n",
       "      <td>https://openreview.net/pdf/84d13359f1520c521f1...</td>\n",
       "      <td>Pay Attention to Features, Transfer Learn Fast...</td>\n",
       "      <td>Deep convolutional neural networks are now wid...</td>\n",
       "      <td>[{\"rating\": \"6: Weak Accept\", \"review\": \" This...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper introduces Attentiv...</td>\n",
       "      <td>{\"decision\": \"Accept (Poster)\", \"comment\": \"Th...</td>\n",
       "      <td>{\"binary_decision\": \"accept\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ryxtWgSKPB</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=ryxtWgSKPB</td>\n",
       "      <td>https://openreview.net/pdf/e90a912a0f6b4596f6b...</td>\n",
       "      <td>Quantum Optical Experiments Modeled by Long Sh...</td>\n",
       "      <td>We demonstrate how machine learning is able to...</td>\n",
       "      <td>[{\"rating\": \"3: Weak Reject\", \"review\": \"This ...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper proposes using mach...</td>\n",
       "      <td>{\"decision\": \"Reject\", \"comment\": \"The paper p...</td>\n",
       "      <td>{\"binary_decision\": \"reject\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ryxtCpNtDS</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=ryxtCpNtDS</td>\n",
       "      <td>https://openreview.net/pdf/7d53de57a3ebd9cc3bb...</td>\n",
       "      <td>Autoencoders and Generative Adversarial Networ...</td>\n",
       "      <td>We introduce a novel synthetic oversampling me...</td>\n",
       "      <td>[{\"rating\": \"3: Weak Reject\", \"review\": \"The p...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper addresses an import...</td>\n",
       "      <td>{\"decision\": \"Reject\", \"comment\": \"This paper ...</td>\n",
       "      <td>{\"binary_decision\": \"reject\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ryxsUySFwr</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=ryxsUySFwr</td>\n",
       "      <td>https://openreview.net/pdf/89002f34556ae166746...</td>\n",
       "      <td>Neural Network Out-of-Distribution Detection f...</td>\n",
       "      <td>Neural network out-of-distribution (OOD) detec...</td>\n",
       "      <td>[{\"rating\": \"3: Weak Reject\", \"review\": \"The a...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper presents an empiric...</td>\n",
       "      <td>{\"decision\": \"Reject\", \"comment\": \"The paper i...</td>\n",
       "      <td>{\"binary_decision\": \"reject\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  submission_id  year                             openreview_link  \\\n",
       "0    ryxz8CVYDH  2020  https://openreview.net/forum?id=ryxz8CVYDH   \n",
       "1    ryxyCeHtPB  2020  https://openreview.net/forum?id=ryxyCeHtPB   \n",
       "2    ryxtWgSKPB  2020  https://openreview.net/forum?id=ryxtWgSKPB   \n",
       "3    ryxtCpNtDS  2020  https://openreview.net/forum?id=ryxtCpNtDS   \n",
       "4    ryxsUySFwr  2020  https://openreview.net/forum?id=ryxsUySFwr   \n",
       "\n",
       "                                   pdf_download_link  \\\n",
       "0  https://openreview.net/pdf/07a4b4b413b37f2c43c...   \n",
       "1  https://openreview.net/pdf/84d13359f1520c521f1...   \n",
       "2  https://openreview.net/pdf/e90a912a0f6b4596f6b...   \n",
       "3  https://openreview.net/pdf/7d53de57a3ebd9cc3bb...   \n",
       "4  https://openreview.net/pdf/89002f34556ae166746...   \n",
       "\n",
       "                                               title  \\\n",
       "0           Learning to Learn by Zeroth-Order Oracle   \n",
       "1  Pay Attention to Features, Transfer Learn Fast...   \n",
       "2  Quantum Optical Experiments Modeled by Long Sh...   \n",
       "3  Autoencoders and Generative Adversarial Networ...   \n",
       "4  Neural Network Out-of-Distribution Detection f...   \n",
       "\n",
       "                                   original_abstract  \\\n",
       "0  In the learning to learn (L2L) framework, we c...   \n",
       "1  Deep convolutional neural networks are now wid...   \n",
       "2  We demonstrate how machine learning is able to...   \n",
       "3  We introduce a novel synthetic oversampling me...   \n",
       "4  Neural network out-of-distribution (OOD) detec...   \n",
       "\n",
       "                                    original_reviews  \\\n",
       "0  [{\"rating\": \"6: Weak Accept\", \"review\": \"The p...   \n",
       "1  [{\"rating\": \"6: Weak Accept\", \"review\": \" This...   \n",
       "2  [{\"rating\": \"3: Weak Reject\", \"review\": \"This ...   \n",
       "3  [{\"rating\": \"3: Weak Reject\", \"review\": \"The p...   \n",
       "4  [{\"rating\": \"3: Weak Reject\", \"review\": \"The a...   \n",
       "\n",
       "                                  normalized_reviews  \\\n",
       "0  [\"{\\\"summary\\\":\\\"The paper introduces a zeroth...   \n",
       "1  [\"{\\\"summary\\\":\\\"The paper introduces Attentiv...   \n",
       "2  [\"{\\\"summary\\\":\\\"The paper proposes using mach...   \n",
       "3  [\"{\\\"summary\\\":\\\"The paper addresses an import...   \n",
       "4  [\"{\\\"summary\\\":\\\"The paper presents an empiric...   \n",
       "\n",
       "                                 original_metareview  \\\n",
       "0  {\"decision\": \"Accept (Poster)\", \"comment\": \"Th...   \n",
       "1  {\"decision\": \"Accept (Poster)\", \"comment\": \"Th...   \n",
       "2  {\"decision\": \"Reject\", \"comment\": \"The paper p...   \n",
       "3  {\"decision\": \"Reject\", \"comment\": \"This paper ...   \n",
       "4  {\"decision\": \"Reject\", \"comment\": \"The paper i...   \n",
       "\n",
       "                                technical_indicators  \n",
       "0  {\"binary_decision\": \"accept\", \"specific_decisi...  \n",
       "1  {\"binary_decision\": \"accept\", \"specific_decisi...  \n",
       "2  {\"binary_decision\": \"reject\", \"specific_decisi...  \n",
       "3  {\"binary_decision\": \"reject\", \"specific_decisi...  \n",
       "4  {\"binary_decision\": \"reject\", \"specific_decisi...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "\n",
    "# splits = [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\", \"2026\"]\n",
    "splits = [\"2020\"]\n",
    "\n",
    "columns_to_keep = [\n",
    "    'submission_id',\n",
    "    'year',\n",
    "    'openreview_link',\n",
    "    'pdf_download_link',\n",
    "    'title',\n",
    "    'original_abstract',\n",
    "    'original_reviews',\n",
    "    'normalized_reviews',\n",
    "    'original_metareview',\n",
    "    'technical_indicators'\n",
    "]\n",
    "\n",
    "streams = [\n",
    "    load_dataset(\n",
    "        \"skonan/iclr-reviews-2020-2026\",\n",
    "        split=s,\n",
    "        streaming=True\n",
    "    )\n",
    "    for s in splits\n",
    "]\n",
    "\n",
    "df_with_rebuttal = pd.DataFrame(\n",
    "    {col: row.get(col) for col in columns_to_keep}\n",
    "    for row in chain.from_iterable(streams)\n",
    ")\n",
    "\n",
    "df_with_rebuttal.head() # dataframe from auto reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "Total number of papers: 55,906\n",
      "Number of columns: 10\n",
      "\n",
      "Shape: (55906, 10)\n"
     ]
    }
   ],
   "source": [
    "# parquet in iclrdataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the parquet file\n",
    "df = pd.read_parquet('../data/iclr26v1.parquet')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total number of papers: {len(df):,}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1678e81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'id', 'title', 'abstract', 'authors', 'author_ids', 'decision',\n",
       "       'scores', 'keywords', 'labels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "901644d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['submission_id', 'year', 'openreview_link', 'pdf_download_link',\n",
       "       'title', 'original_abstract', 'original_reviews', 'normalized_reviews',\n",
       "       'original_metareview', 'technical_indicators'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_rebuttal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef723285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original df shape: (55906, 10)\n",
      "df_with_rebuttal shape: (2198, 10)\n",
      "Merged df shape: (2198, 20)\n",
      "\n",
      "Columns in merged df: ['year_x', 'id', 'title_x', 'abstract', 'authors', 'author_ids', 'decision', 'scores', 'keywords', 'labels', 'submission_id', 'year_y', 'openreview_link', 'pdf_download_link', 'title_y', 'original_abstract', 'original_reviews', 'normalized_reviews', 'original_metareview', 'technical_indicators']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_x</th>\n",
       "      <th>id</th>\n",
       "      <th>title_x</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_ids</th>\n",
       "      <th>decision</th>\n",
       "      <th>scores</th>\n",
       "      <th>keywords</th>\n",
       "      <th>labels</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>year_y</th>\n",
       "      <th>openreview_link</th>\n",
       "      <th>pdf_download_link</th>\n",
       "      <th>title_y</th>\n",
       "      <th>original_abstract</th>\n",
       "      <th>original_reviews</th>\n",
       "      <th>normalized_reviews</th>\n",
       "      <th>original_metareview</th>\n",
       "      <th>technical_indicators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>B1e-kxSKDH</td>\n",
       "      <td>Structured Object-Aware Physics Prediction for...</td>\n",
       "      <td>When humans observe a physical system, they ca...</td>\n",
       "      <td>Jannik Kossen, Karl Stelzner, Marcel Hussing, ...</td>\n",
       "      <td></td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[6, 6, 6]</td>\n",
       "      <td>[self-supervised learning, probabilistic deep ...</td>\n",
       "      <td>autoencoders</td>\n",
       "      <td>B1e-kxSKDH</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=B1e-kxSKDH</td>\n",
       "      <td>https://openreview.net/pdf/28defda144549878c89...</td>\n",
       "      <td>Structured Object-Aware Physics Prediction for...</td>\n",
       "      <td>When humans observe a physical system, they ca...</td>\n",
       "      <td>[{\"rating\": \"6: Weak Accept\", \"review\": \"This ...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper presents a structur...</td>\n",
       "      <td>{\"decision\": \"Accept (Poster)\", \"comment\": \"Th...</td>\n",
       "      <td>{\"binary_decision\": \"accept\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>B1e3OlStPB</td>\n",
       "      <td>DeepSphere: a graph-based spherical CNN</td>\n",
       "      <td>Designing a convolution for a spherical neural...</td>\n",
       "      <td>Michaël Defferrard, Martino Milani, Frédérick ...</td>\n",
       "      <td></td>\n",
       "      <td>Accept (Spotlight)</td>\n",
       "      <td>[8, 6, 6]</td>\n",
       "      <td>[spherical cnns, graph neural networks, geomet...</td>\n",
       "      <td>graphs</td>\n",
       "      <td>B1e3OlStPB</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=B1e3OlStPB</td>\n",
       "      <td>https://openreview.net/pdf/1557f75b9013011ccab...</td>\n",
       "      <td>DeepSphere: a graph-based spherical CNN</td>\n",
       "      <td>Designing a convolution for a spherical neural...</td>\n",
       "      <td>[{\"rating\": \"6: Weak Accept\", \"review\": \"In th...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper studies CNNs specia...</td>\n",
       "      <td>{\"decision\": \"Accept (Spotlight)\", \"comment\": ...</td>\n",
       "      <td>{\"binary_decision\": \"accept\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>B1e5TA4FPr</td>\n",
       "      <td>Pareto Optimality in No-Harm Fairness</td>\n",
       "      <td>Common fairness definitions in machine learnin...</td>\n",
       "      <td>Natalia Martinez, Martin Bertran, Guillermo Sa...</td>\n",
       "      <td></td>\n",
       "      <td>Reject</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "      <td>[fairness, fairness in machine learning, no-ha...</td>\n",
       "      <td>fairness</td>\n",
       "      <td>B1e5TA4FPr</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=B1e5TA4FPr</td>\n",
       "      <td>https://openreview.net/pdf/141431619907795b4f2...</td>\n",
       "      <td>Pareto Optimality in No-Harm Fairness</td>\n",
       "      <td>Common fairness definitions in machine learnin...</td>\n",
       "      <td>[{\"rating\": \"3: Weak Reject\", \"review\": \"This ...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper proposes a fairness...</td>\n",
       "      <td>{\"decision\": \"Reject\", \"comment\": \"This manusc...</td>\n",
       "      <td>{\"binary_decision\": \"reject\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>B1e9Y2NYvS</td>\n",
       "      <td>On Robustness of Neural Ordinary Differential ...</td>\n",
       "      <td>Neural ordinary differential equations (ODEs) ...</td>\n",
       "      <td>Hanshu YAN, Jiawei DU, Vincent TAN, Jiashi FENG</td>\n",
       "      <td></td>\n",
       "      <td>Accept (Spotlight)</td>\n",
       "      <td>[6, 8, 6]</td>\n",
       "      <td>[neural ode]</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>B1e9Y2NYvS</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=B1e9Y2NYvS</td>\n",
       "      <td>https://openreview.net/pdf/48923b13b7666cc0e57...</td>\n",
       "      <td>On Robustness of Neural Ordinary Differential ...</td>\n",
       "      <td>Neural ordinary differential equations (ODEs)...</td>\n",
       "      <td>[{\"rating\": \"6: Weak Accept\", \"review\": \"This ...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The paper investigates the ro...</td>\n",
       "      <td>{\"decision\": \"Accept (Spotlight)\", \"comment\": ...</td>\n",
       "      <td>{\"binary_decision\": \"accept\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>B1eB5xSFvr</td>\n",
       "      <td>DiffTaichi: Differentiable Programming for Phy...</td>\n",
       "      <td>We present DiffTaichi, a new differentiable pr...</td>\n",
       "      <td>Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun...</td>\n",
       "      <td></td>\n",
       "      <td>Accept (Poster)</td>\n",
       "      <td>[6, 3, 6]</td>\n",
       "      <td>[differentiable programming, robotics, optimal...</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>B1eB5xSFvr</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://openreview.net/forum?id=B1eB5xSFvr</td>\n",
       "      <td>https://openreview.net/pdf/bee5278794e8dc780a2...</td>\n",
       "      <td>DiffTaichi: Differentiable Programming for Phy...</td>\n",
       "      <td>We present DiffTaichi, a new differentiable pr...</td>\n",
       "      <td>[{\"rating\": \"6: Weak Accept\", \"review\": \"This ...</td>\n",
       "      <td>[\"{\\\"summary\\\":\\\"The reviewer acknowledges the...</td>\n",
       "      <td>{\"decision\": \"Accept (Poster)\", \"comment\": \"Th...</td>\n",
       "      <td>{\"binary_decision\": \"accept\", \"specific_decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_x          id                                            title_x  \\\n",
       "0    2020  B1e-kxSKDH  Structured Object-Aware Physics Prediction for...   \n",
       "1    2020  B1e3OlStPB            DeepSphere: a graph-based spherical CNN   \n",
       "2    2020  B1e5TA4FPr              Pareto Optimality in No-Harm Fairness   \n",
       "3    2020  B1e9Y2NYvS  On Robustness of Neural Ordinary Differential ...   \n",
       "4    2020  B1eB5xSFvr  DiffTaichi: Differentiable Programming for Phy...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  When humans observe a physical system, they ca...   \n",
       "1  Designing a convolution for a spherical neural...   \n",
       "2  Common fairness definitions in machine learnin...   \n",
       "3  Neural ordinary differential equations (ODEs) ...   \n",
       "4  We present DiffTaichi, a new differentiable pr...   \n",
       "\n",
       "                                             authors author_ids  \\\n",
       "0  Jannik Kossen, Karl Stelzner, Marcel Hussing, ...              \n",
       "1  Michaël Defferrard, Martino Milani, Frédérick ...              \n",
       "2  Natalia Martinez, Martin Bertran, Guillermo Sa...              \n",
       "3    Hanshu YAN, Jiawei DU, Vincent TAN, Jiashi FENG              \n",
       "4  Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun...              \n",
       "\n",
       "             decision     scores  \\\n",
       "0     Accept (Poster)  [6, 6, 6]   \n",
       "1  Accept (Spotlight)  [8, 6, 6]   \n",
       "2              Reject  [3, 3, 3]   \n",
       "3  Accept (Spotlight)  [6, 8, 6]   \n",
       "4     Accept (Poster)  [6, 3, 6]   \n",
       "\n",
       "                                            keywords        labels  \\\n",
       "0  [self-supervised learning, probabilistic deep ...  autoencoders   \n",
       "1  [spherical cnns, graph neural networks, geomet...        graphs   \n",
       "2  [fairness, fairness in machine learning, no-ha...      fairness   \n",
       "3                                       [neural ode]     unlabeled   \n",
       "4  [differentiable programming, robotics, optimal...     unlabeled   \n",
       "\n",
       "  submission_id  year_y                             openreview_link  \\\n",
       "0    B1e-kxSKDH    2020  https://openreview.net/forum?id=B1e-kxSKDH   \n",
       "1    B1e3OlStPB    2020  https://openreview.net/forum?id=B1e3OlStPB   \n",
       "2    B1e5TA4FPr    2020  https://openreview.net/forum?id=B1e5TA4FPr   \n",
       "3    B1e9Y2NYvS    2020  https://openreview.net/forum?id=B1e9Y2NYvS   \n",
       "4    B1eB5xSFvr    2020  https://openreview.net/forum?id=B1eB5xSFvr   \n",
       "\n",
       "                                   pdf_download_link  \\\n",
       "0  https://openreview.net/pdf/28defda144549878c89...   \n",
       "1  https://openreview.net/pdf/1557f75b9013011ccab...   \n",
       "2  https://openreview.net/pdf/141431619907795b4f2...   \n",
       "3  https://openreview.net/pdf/48923b13b7666cc0e57...   \n",
       "4  https://openreview.net/pdf/bee5278794e8dc780a2...   \n",
       "\n",
       "                                             title_y  \\\n",
       "0  Structured Object-Aware Physics Prediction for...   \n",
       "1            DeepSphere: a graph-based spherical CNN   \n",
       "2              Pareto Optimality in No-Harm Fairness   \n",
       "3  On Robustness of Neural Ordinary Differential ...   \n",
       "4  DiffTaichi: Differentiable Programming for Phy...   \n",
       "\n",
       "                                   original_abstract  \\\n",
       "0  When humans observe a physical system, they ca...   \n",
       "1  Designing a convolution for a spherical neural...   \n",
       "2  Common fairness definitions in machine learnin...   \n",
       "3   Neural ordinary differential equations (ODEs)...   \n",
       "4  We present DiffTaichi, a new differentiable pr...   \n",
       "\n",
       "                                    original_reviews  \\\n",
       "0  [{\"rating\": \"6: Weak Accept\", \"review\": \"This ...   \n",
       "1  [{\"rating\": \"6: Weak Accept\", \"review\": \"In th...   \n",
       "2  [{\"rating\": \"3: Weak Reject\", \"review\": \"This ...   \n",
       "3  [{\"rating\": \"6: Weak Accept\", \"review\": \"This ...   \n",
       "4  [{\"rating\": \"6: Weak Accept\", \"review\": \"This ...   \n",
       "\n",
       "                                  normalized_reviews  \\\n",
       "0  [\"{\\\"summary\\\":\\\"The paper presents a structur...   \n",
       "1  [\"{\\\"summary\\\":\\\"The paper studies CNNs specia...   \n",
       "2  [\"{\\\"summary\\\":\\\"The paper proposes a fairness...   \n",
       "3  [\"{\\\"summary\\\":\\\"The paper investigates the ro...   \n",
       "4  [\"{\\\"summary\\\":\\\"The reviewer acknowledges the...   \n",
       "\n",
       "                                 original_metareview  \\\n",
       "0  {\"decision\": \"Accept (Poster)\", \"comment\": \"Th...   \n",
       "1  {\"decision\": \"Accept (Spotlight)\", \"comment\": ...   \n",
       "2  {\"decision\": \"Reject\", \"comment\": \"This manusc...   \n",
       "3  {\"decision\": \"Accept (Spotlight)\", \"comment\": ...   \n",
       "4  {\"decision\": \"Accept (Poster)\", \"comment\": \"Th...   \n",
       "\n",
       "                                technical_indicators  \n",
       "0  {\"binary_decision\": \"accept\", \"specific_decisi...  \n",
       "1  {\"binary_decision\": \"accept\", \"specific_decisi...  \n",
       "2  {\"binary_decision\": \"reject\", \"specific_decisi...  \n",
       "3  {\"binary_decision\": \"accept\", \"specific_decisi...  \n",
       "4  {\"binary_decision\": \"accept\", \"specific_decisi...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inner join: df has `id` column, df_with_rebuttal has `submission_id` column\n",
    "# Common columns: 'year' and 'title' - we'll keep them from df and drop from df_with_rebuttal\n",
    "\n",
    "# Identify common columns (excluding the join keys)\n",
    "common_cols = set(df.columns) & set(df_with_rebuttal.columns)\n",
    "# Remove the join key columns from common columns\n",
    "common_cols = common_cols - {'id', 'submission_id'}\n",
    "print(f\"Common columns (will keep from df): {common_cols}\")\n",
    "\n",
    "# Drop common columns from df_with_rebuttal to avoid _x/_y suffixes\n",
    "df_with_rebuttal_clean = df_with_rebuttal.drop(columns=list(common_cols))\n",
    "print(f\"\\ndf_with_rebuttal columns after dropping duplicates: {list(df_with_rebuttal_clean.columns)}\")\n",
    "\n",
    "# Merge: use left_on and right_on to specify different column names\n",
    "df_merged = pd.merge(\n",
    "    df, \n",
    "    df_with_rebuttal_clean, \n",
    "    left_on='id',           # Column name in df\n",
    "    right_on='submission_id',  # Column name in df_with_rebuttal\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal df shape: {df.shape}\")\n",
    "print(f\"df_with_rebuttal shape: {df_with_rebuttal.shape}\")\n",
    "print(f\"Merged df shape: {df_merged.shape}\")\n",
    "print(f\"\\nColumns in merged df ({len(df_merged.columns)}): {list(df_merged.columns)}\")\n",
    "\n",
    "df_merged.head()\n",
    "\n",
    "# save to parquet\n",
    "df_merged.to_parquet('../data/iclr26v1_with_rebuttal.parquet')\n",
    "print(f\"\\nSaved merged dataframe to '../data/iclr26v1_with_rebuttal.parquet'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af1763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "unlabeled                     974\n",
       "RL                            168\n",
       "adversarial                   118\n",
       "optimization                   74\n",
       "graphs                         68\n",
       "GANs                           67\n",
       "language models                60\n",
       "autoencoders                   52\n",
       "transformers                   41\n",
       "RNNs                           40\n",
       "CNNs                           39\n",
       "meta learning                  38\n",
       "transfer learning              35\n",
       "LLMs                           27\n",
       "continual learning             27\n",
       "imitation learning             22\n",
       "neural architecture search     22\n",
       "compression                    22\n",
       "interpretability               19\n",
       "multi-agent RL                 19\n",
       "out-of-distribution            17\n",
       "self-supervised learning       17\n",
       "semi-supervised learning       16\n",
       "robustness                     16\n",
       "pruning                        16\n",
       "few-shot learning              15\n",
       "multi-task learning            13\n",
       "model-based RL                 12\n",
       "speech                         10\n",
       "explainability                 10\n",
       "object detection               10\n",
       "active learning                 9\n",
       "anomaly detection               9\n",
       "code generation                 9\n",
       "optimal transport               8\n",
       "privacy                         8\n",
       "clustering                      8\n",
       "PDEs                            7\n",
       "federated learning              7\n",
       "knowledge distillation          7\n",
       "knowledge graphs                6\n",
       "causality                       6\n",
       "safety                          5\n",
       "autonomous driving              4\n",
       "neuroscience                    4\n",
       "time series                     4\n",
       "fairness                        3\n",
       "molecules                       3\n",
       "3D scenes                       3\n",
       "offline RL                      2\n",
       "alignment                       1\n",
       "diffusion models                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['labels'].value_counts() # 74 papers on optimization in 2020, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cff0fb04",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/iclr-dataset/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'year'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get 10 random papers on optimiza  tion from 2020\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimization_2020 \u001b[38;5;241m=\u001b[39m df_merged[(df_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimization\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[43mdf_merged\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2020\u001b[39m)]\n\u001b[1;32m      3\u001b[0m random_papers \u001b[38;5;241m=\u001b[39m optimization_2020\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m random_papers\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.conda/envs/iclr-dataset/lib/python3.9/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/iclr-dataset/lib/python3.9/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "# get 10 random papers on optimiza  tion from 2020\n",
    "optimization_2020 = df_merged[(df_merged['labels'] == 'optimization') & (df_merged['year'] == 2020)]\n",
    "random_papers = optimization_2020.sample(n=10, random_state=42)\n",
    "random_papers.head()\n",
    "\n",
    "# get reviews for the papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0938874a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCHEMA / COLUMN INFORMATION\n",
      "============================================================\n",
      "\n",
      "Column names and data types:\n",
      "year           int64\n",
      "id            object\n",
      "title         object\n",
      "abstract      object\n",
      "authors       object\n",
      "author_ids    object\n",
      "decision      object\n",
      "scores        object\n",
      "keywords      object\n",
      "labels        object\n",
      "dtype: object\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Column details:\n",
      "\n",
      "year:\n",
      "  Type: int64\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "id:\n",
      "  Type: object\n",
      "  Sample value: B1-Hhnslg\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "title:\n",
      "  Type: object\n",
      "  Sample value: Prototypical Networks for Few-shot Learning\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "abstract:\n",
      "  Type: object\n",
      "  Sample value: A recent approach to few-shot classification called matching networks has demonstrated the benefits \n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "authors:\n",
      "  Type: object\n",
      "  Sample value: Jake Snell, Kevin Swersky, Richard Zemel\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "author_ids:\n",
      "  Type: object\n",
      "  Sample value: \n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "decision:\n",
      "  Type: object\n",
      "  Sample value: Reject\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "scores:\n",
      "  Type: object\n",
      "  Sample value: [6 4 5]\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "keywords:\n",
      "  Type: object\n",
      "  Sample value: ['deep learning' 'transfer learning']\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n",
      "\n",
      "labels:\n",
      "  Type: object\n",
      "  Sample value: transfer learning\n",
      "  Non-null count: 55,906 / 55,906\n",
      "  Null count: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SCHEMA / COLUMN INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nColumn names and data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nColumn details:\")\n",
    "for col in df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Type: {df[col].dtype}\")\n",
    "    if df[col].dtype == 'object':\n",
    "        # For object types, show sample values\n",
    "        sample = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else None\n",
    "        if isinstance(sample, list):\n",
    "            print(f\"  Sample value: {sample[:3] if len(sample) > 3 else sample} (list with {len(sample)} items)\")\n",
    "        else:\n",
    "            print(f\"  Sample value: {str(sample)[:100] if sample is not None else 'N/A'}\")\n",
    "    print(f\"  Non-null count: {df[col].notna().sum():,} / {len(df):,}\")\n",
    "    print(f\"  Null count: {df[col].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ab2f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PAPERS BY YEAR\n",
      "============================================================\n",
      "\n",
      "Number of papers per year:\n",
      "year\n",
      "2017      489\n",
      "2018     1012\n",
      "2019     1569\n",
      "2020     2593\n",
      "2021     3009\n",
      "2022     3422\n",
      "2023     4955\n",
      "2024     7401\n",
      "2025    11663\n",
      "2026    19793\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total: 55,906 papers\n",
      "\n",
      "Year range: 2017 - 2026\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PAPERS BY YEAR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'year' in df.columns:\n",
    "    year_counts = df['year'].value_counts().sort_index()\n",
    "    print(\"\\nNumber of papers per year:\")\n",
    "    print(year_counts)\n",
    "    print(f\"\\nTotal: {year_counts.sum():,} papers\")\n",
    "    print(f\"\\nYear range: {df['year'].min()} - {df['year'].max()}\")\n",
    "    \n",
    "    # Visualize\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    year_counts.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Number of Papers', fontsize=12)\n",
    "    plt.title('Number of Papers per Year', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"'year' column not found in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ad243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASIC STATISTICS\n",
      "============================================================\n",
      "\n",
      "First few rows:\n",
      "   year         id                                              title  \\\n",
      "0  2017  B1-Hhnslg        Prototypical Networks for Few-shot Learning   \n",
      "1  2017  B1-q5Pqxl  Machine Comprehension Using Match-LSTM and Ans...   \n",
      "2  2017  B16Jem9xe             Learning in Implicit Generative Models   \n",
      "3  2017  B16dGcqlx                    Third Person Imitation Learning   \n",
      "4  2017  B184E5qee  Improving Neural Language Models with a Contin...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  A recent approach to few-shot classification c...   \n",
      "1  Machine comprehension of text is an important ...   \n",
      "2  Generative adversarial networks (GANs) provide...   \n",
      "3  Reinforcement learning (RL) makes it possible ...   \n",
      "4  We propose an extension to neural network lang...   \n",
      "\n",
      "                                          authors author_ids  \\\n",
      "0        Jake Snell, Kevin Swersky, Richard Zemel              \n",
      "1                       Shuohang Wang, Jing Jiang              \n",
      "2         Shakir Mohamed, Balaji Lakshminarayanan              \n",
      "3  Bradly C Stadie, Pieter Abbeel, Ilya Sutskever              \n",
      "4   Edouard Grave, Armand Joulin, Nicolas Usunier              \n",
      "\n",
      "                   decision     scores  \\\n",
      "0                    Reject  [6, 4, 5]   \n",
      "1           Accept (Poster)  [6, 6, 7]   \n",
      "2  Invite to Workshop Track  [8, 7, 6]   \n",
      "3           Accept (Poster)  [6, 5, 6]   \n",
      "4           Accept (Poster)  [7, 9, 5]   \n",
      "\n",
      "                                       keywords             labels  \n",
      "0            [deep learning, transfer learning]  transfer learning  \n",
      "1  [natural language processing, deep learning]    language models  \n",
      "2                       [unsupervised learning]          unlabeled  \n",
      "3                                            []          unlabeled  \n",
      "4                 [natural language processing]    language models  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55906 entries, 0 to 55905\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   year        55906 non-null  int64 \n",
      " 1   id          55906 non-null  object\n",
      " 2   title       55906 non-null  object\n",
      " 3   abstract    55906 non-null  object\n",
      " 4   authors     55906 non-null  object\n",
      " 5   author_ids  55906 non-null  object\n",
      " 6   decision    55906 non-null  object\n",
      " 7   scores      55906 non-null  object\n",
      " 8   keywords    55906 non-null  object\n",
      " 9   labels      55906 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"\\nDataFrame info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f600638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDITIONAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Decision distribution:\n",
      "decision\n",
      "                            19673\n",
      "Reject                      17096\n",
      "Withdrawn                    7594\n",
      "Accept (Poster)              6173\n",
      "Accept (poster)              1809\n",
      "Accept: poster               1202\n",
      "Accept (Spotlight)            775\n",
      "Accept (Oral)                 383\n",
      "Accept (spotlight)            366\n",
      "Accept: notable-top-25%       280\n",
      "Desk rejected                 194\n",
      "Invite to Workshop Track      136\n",
      "Accept: notable-top-5%         91\n",
      "Accept (oral)                  86\n",
      "Accept (Talk)                  48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Acceptance rate (if 'Accept' in decisions):\n",
      "  Accepted: 11,213 (20.1%)\n",
      "  Total: 55,906\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Score statistics:\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Abstract length statistics:\n",
      "  Mean length: 1325.1 characters\n",
      "  Median length: 1313.0 characters\n",
      "  Min length: 108 characters\n",
      "  Max length: 4999 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ADDITIONAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for decision column\n",
    "if 'decision' in df.columns:\n",
    "    print(\"\\nDecision distribution:\")\n",
    "    decision_counts = df['decision'].value_counts()\n",
    "    print(decision_counts)\n",
    "    print(f\"\\nAcceptance rate (if 'Accept' in decisions):\")\n",
    "    if any('Accept' in str(d) for d in decision_counts.index):\n",
    "        accept_cols = [d for d in decision_counts.index if 'Accept' in str(d)]\n",
    "        accept_count = sum(decision_counts[d] for d in accept_cols)\n",
    "        print(f\"  Accepted: {accept_count:,} ({100*accept_count/len(df):.1f}%)\")\n",
    "        print(f\"  Total: {len(df):,}\")\n",
    "\n",
    "# Check for scores column\n",
    "if 'scores' in df.columns:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"\\nScore statistics:\")\n",
    "    # Calculate average scores for papers with scores\n",
    "    avg_scores = []\n",
    "    for scores in df['scores']:\n",
    "        if isinstance(scores, list) and len(scores) > 0:\n",
    "            avg_scores.append(np.mean(scores))\n",
    "    \n",
    "    if avg_scores:\n",
    "        print(f\"  Papers with scores: {len(avg_scores):,}\")\n",
    "        print(f\"  Average score: {np.mean(avg_scores):.2f}\")\n",
    "        print(f\"  Median score: {np.median(avg_scores):.2f}\")\n",
    "        print(f\"  Min score: {np.min(avg_scores):.2f}\")\n",
    "        print(f\"  Max score: {np.max(avg_scores):.2f}\")\n",
    "\n",
    "# Check abstract lengths\n",
    "if 'abstract' in df.columns:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"\\nAbstract length statistics:\")\n",
    "    abstract_lengths = df['abstract'].str.len()\n",
    "    print(f\"  Mean length: {abstract_lengths.mean():.1f} characters\")\n",
    "    print(f\"  Median length: {abstract_lengths.median():.1f} characters\")\n",
    "    print(f\"  Min length: {abstract_lengths.min()} characters\")\n",
    "    print(f\"  Max length: {abstract_lengths.max()} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5036911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE DATA\n",
      "============================================================\n",
      "\n",
      "Sample rows (showing columns: year, title, abstract):\n",
      "   year                                                      title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         abstract\n",
      "0  2017                Prototypical Networks for Few-shot Learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset.\n",
      "1  2017  Machine Comprehension Using Match-LSTM and Answer Pointer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our tasks. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features. Besides, our boundary model also achieves the best performance on the MSMARCO dataset (Nguyen et al. 2016).\n",
      "2  2017                     Learning in Implicit Generative Models  Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop  our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models---models that only specify a stochastic procedure with which to generate data---and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding  in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show a few sample rows with key information\n",
    "sample_cols = ['year', 'title', 'abstract'] if all(c in df.columns for c in ['year', 'title', 'abstract']) else df.columns[:5]\n",
    "print(f\"\\nSample rows (showing columns: {', '.join(sample_cols)}):\")\n",
    "print(df[sample_cols].head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89127575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [deep learning, transfer learning]\n",
       "1             [natural language processing, deep learning]\n",
       "2                                  [unsupervised learning]\n",
       "3                                                       []\n",
       "4                            [natural language processing]\n",
       "                               ...                        \n",
       "55901    [deep neural networks, activation function lea...\n",
       "55902    [diffusion models, distribution matching, dist...\n",
       "55903                                   [sparse attention]\n",
       "55904                [reinforcement learning, exploration]\n",
       "55905            [large language models, persona modeling]\n",
       "Name: keywords, Length: 55906, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcbb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['transfer learning', 'language models', 'unlabeled',\n",
       "       'optimization', 'semi-supervised learning', 'RL',\n",
       "       'multi-task learning', 'GANs', 'RNNs', 'clustering',\n",
       "       'code generation', 'autoencoders', 'few-shot learning',\n",
       "       'multi-agent RL', 'LLMs', 'neuroscience', 'compression',\n",
       "       'continual learning', 'CNNs', 'speech', 'adversarial', 'privacy',\n",
       "       'imitation learning', 'transformers', 'graphs', 'meta learning',\n",
       "       'neural architecture search', 'robustness', 'optimal transport',\n",
       "       'object detection', 'out-of-distribution', 'interpretability',\n",
       "       'active learning', 'causality', 'pruning', 'safety',\n",
       "       'model-based RL', 'fairness', 'knowledge graphs',\n",
       "       'diffusion models', 'PDEs', 'alignment', 'anomaly detection',\n",
       "       'time series', 'explainability', 'self-supervised learning',\n",
       "       'knowledge distillation', 'federated learning', 'molecules',\n",
       "       'autonomous driving', '3D scenes', 'offline RL', 'ViTs',\n",
       "       'vision-language models', 'in-context learning'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].unique() # all the unique areas in ICLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c92e5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LABEL COUNTS\n",
      "============================================================\n",
      "\n",
      "Count of papers per label:\n",
      "labels\n",
      "unlabeled                     25371\n",
      "LLMs                           4116\n",
      "RL                             2178\n",
      "language models                1773\n",
      "diffusion models               1749\n",
      "graphs                         1404\n",
      "optimization                   1382\n",
      "adversarial                    1310\n",
      "transformers                   1048\n",
      "self-supervised learning        857\n",
      "transfer learning               664\n",
      "continual learning              567\n",
      "federated learning              567\n",
      "vision-language models          553\n",
      "out-of-distribution             485\n",
      "autoencoders                    459\n",
      "causality                       454\n",
      "GANs                            453\n",
      "privacy                         451\n",
      "time series                     439\n",
      "safety                          438\n",
      "interpretability                426\n",
      "alignment                       411\n",
      "explainability                  408\n",
      "compression                     399\n",
      "multi-agent RL                  390\n",
      "meta learning                   387\n",
      "RNNs                            357\n",
      "offline RL                      353\n",
      "CNNs                            337\n",
      "robustness                      335\n",
      "3D scenes                       331\n",
      "object detection                305\n",
      "knowledge distillation          302\n",
      "in-context learning             296\n",
      "PDEs                            291\n",
      "code generation                 267\n",
      "imitation learning              261\n",
      "fairness                        251\n",
      "optimal transport               249\n",
      "pruning                         237\n",
      "semi-supervised learning        230\n",
      "ViTs                            229\n",
      "few-shot learning               222\n",
      "multi-task learning             211\n",
      "clustering                      195\n",
      "autonomous driving              192\n",
      "active learning                 191\n",
      "knowledge graphs                190\n",
      "anomaly detection               188\n",
      "neural architecture search      182\n",
      "model-based RL                  156\n",
      "neuroscience                    141\n",
      "molecules                       137\n",
      "speech                          131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total labels: 55\n",
      "Total papers: 55,906\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Label distribution (percentage):\n",
      "  unlabeled: 25,371 (45.38%)\n",
      "  LLMs: 4,116 (7.36%)\n",
      "  RL: 2,178 (3.90%)\n",
      "  language models: 1,773 (3.17%)\n",
      "  diffusion models: 1,749 (3.13%)\n",
      "  graphs: 1,404 (2.51%)\n",
      "  optimization: 1,382 (2.47%)\n",
      "  adversarial: 1,310 (2.34%)\n",
      "  transformers: 1,048 (1.87%)\n",
      "  self-supervised learning: 857 (1.53%)\n",
      "  transfer learning: 664 (1.19%)\n",
      "  continual learning: 567 (1.01%)\n",
      "  federated learning: 567 (1.01%)\n",
      "  vision-language models: 553 (0.99%)\n",
      "  out-of-distribution: 485 (0.87%)\n",
      "  autoencoders: 459 (0.82%)\n",
      "  causality: 454 (0.81%)\n",
      "  GANs: 453 (0.81%)\n",
      "  privacy: 451 (0.81%)\n",
      "  time series: 439 (0.79%)\n",
      "  safety: 438 (0.78%)\n",
      "  interpretability: 426 (0.76%)\n",
      "  alignment: 411 (0.74%)\n",
      "  explainability: 408 (0.73%)\n",
      "  compression: 399 (0.71%)\n",
      "  multi-agent RL: 390 (0.70%)\n",
      "  meta learning: 387 (0.69%)\n",
      "  RNNs: 357 (0.64%)\n",
      "  offline RL: 353 (0.63%)\n",
      "  CNNs: 337 (0.60%)\n",
      "  robustness: 335 (0.60%)\n",
      "  3D scenes: 331 (0.59%)\n",
      "  object detection: 305 (0.55%)\n",
      "  knowledge distillation: 302 (0.54%)\n",
      "  in-context learning: 296 (0.53%)\n",
      "  PDEs: 291 (0.52%)\n",
      "  code generation: 267 (0.48%)\n",
      "  imitation learning: 261 (0.47%)\n",
      "  fairness: 251 (0.45%)\n",
      "  optimal transport: 249 (0.45%)\n",
      "  pruning: 237 (0.42%)\n",
      "  semi-supervised learning: 230 (0.41%)\n",
      "  ViTs: 229 (0.41%)\n",
      "  few-shot learning: 222 (0.40%)\n",
      "  multi-task learning: 211 (0.38%)\n",
      "  clustering: 195 (0.35%)\n",
      "  autonomous driving: 192 (0.34%)\n",
      "  active learning: 191 (0.34%)\n",
      "  knowledge graphs: 190 (0.34%)\n",
      "  anomaly detection: 188 (0.34%)\n",
      "  neural architecture search: 182 (0.33%)\n",
      "  model-based RL: 156 (0.28%)\n",
      "  neuroscience: 141 (0.25%)\n",
      "  molecules: 137 (0.25%)\n",
      "  speech: 131 (0.23%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LABEL COUNTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'labels' in df.columns:\n",
    "    label_counts = df['labels'].value_counts().sort_values(ascending=False)\n",
    "    print(\"\\nCount of papers per label:\")\n",
    "    print(label_counts)\n",
    "    print(f\"\\nTotal labels: {len(label_counts)}\")\n",
    "    print(f\"Total papers: {label_counts.sum():,}\")\n",
    "    \n",
    "    # Show percentages\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"\\nLabel distribution (percentage):\")\n",
    "    label_pct = (df['labels'].value_counts(normalize=True) * 100).sort_values(ascending=False)\n",
    "    for label, count in label_counts.items():\n",
    "        pct = label_pct[label]\n",
    "        print(f\"  {label}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"'labels' column not found in dataset\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d641fb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "10 RANDOM OPTIMIZATION PAPERS FROM 2020\n",
      "============================================================\n",
      "\n",
      "Total optimization papers from 2020: 80\n",
      "\n",
      "Showing 10 random papers:\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Paper ID: HkgTTh4FDH\n",
      "   Title: Implicit Bias of Gradient Descent based Adversarial Training on Separable Data\n",
      "   Authors: Yan Li, Ethan X.Fang, Huan Xu, Tuo Zhao...\n",
      "   Decision: Accept (Poster)\n",
      "   Abstract: Adversarial training is a principled approach for training robust neural networks. Despite of tremendous successes in practice, its theoretical properties still remain largely unexplored. In this pape...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Paper ID: B1erJJrYPH\n",
      "   Title: Optimizing Loss Landscape Connectivity via Neuron Alignment\n",
      "   Authors: N. Joseph Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, Rongjie Lai...\n",
      "   Decision: Reject\n",
      "   Abstract: The loss landscapes of deep neural networks are poorly understood due to their high nonconvexity. Empirically, the local optima of these loss functions can be connected by a simple curve in model spac...\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Paper ID: H1gEP6NFwr\n",
      "   Title: On the Tunability of Optimizers in Deep Learning\n",
      "   Authors: Prabhu Teja S*, Florian Mai*, Thijs Vogels, Martin Jaggi, Francois Fleuret...\n",
      "   Decision: Reject\n",
      "   Abstract: There is no consensus yet on the question whether adaptive gradient methods like Adam are easier to use than non-adaptive optimization methods like SGD. In this work, we fill in the important, yet amb...\n",
      "------------------------------------------------------------\n",
      "\n",
      "4. Paper ID: Hklcm0VYDS\n",
      "   Title: How noise affects the Hessian spectrum in overparameterized neural networks\n",
      "   Authors: Mingwei Wei, David Schwab...\n",
      "   Decision: Reject\n",
      "   Abstract: Stochastic gradient descent (SGD) forms the core optimization method for deep neural networks. While some theoretical progress has been made, it still remains unclear why SGD leads the learning dynami...\n",
      "------------------------------------------------------------\n",
      "\n",
      "5. Paper ID: BygJKn4tPr\n",
      "   Title: Effective Mechanism to Mitigate Injuries During NFL Plays\n",
      "   Authors: Arraamuthan Arulanantham, Ahamed Arshad Ahamed Anzar, Gowshalini Rajalingam, Krusanth Ingran, Prasanna S. Haddela...\n",
      "   Decision: Reject\n",
      "   Abstract: NFL(American football),which is regarded as the premier sports icon of America, has been severely accused in the recent years of being exposed to dangerous injuries that prove to be a bigger crisis as...\n",
      "------------------------------------------------------------\n",
      "\n",
      "6. Paper ID: HkerqCEtvS\n",
      "   Title: Stochastic Gradient Methods with Block Diagonal Matrix Adaptation\n",
      "   Authors: Jihun Yun, Aurélie C. Lozano, Eunho Yang...\n",
      "   Decision: Desk rejected\n",
      "   Abstract: Adaptive gradient approaches that automatically adjust the learning rate on a per-feature basis have been very popular for training deep networks. This rich class of algorithms includes Adagrad, RMSpr...\n",
      "------------------------------------------------------------\n",
      "\n",
      "7. Paper ID: BJgnXpVYwS\n",
      "   Title: Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity\n",
      "   Authors: Jingzhao Zhang, Tianxing He, Suvrit Sra, Ali Jadbabaie...\n",
      "   Decision: Accept (Talk)\n",
      "   Abstract: We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network...\n",
      "------------------------------------------------------------\n",
      "\n",
      "8. Paper ID: rkgqN1SYvr\n",
      "   Title: Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks\n",
      "   Authors: Wei Hu, Lechao Xiao, Jeffrey Pennington...\n",
      "   Decision: Accept (Poster)\n",
      "   Abstract: The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergen...\n",
      "------------------------------------------------------------\n",
      "\n",
      "9. Paper ID: B1lxV6NFPH\n",
      "   Title: BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search\n",
      "   Authors: Colin White, Willie Neiswanger, Yash Savani...\n",
      "   Decision: Reject\n",
      "   Abstract: Neural Architecture Search (NAS) has seen an explosion of research in the past few years. A variety of methods have been proposed to perform NAS, including reinforcement learning, Bayesian optimizatio...\n",
      "------------------------------------------------------------\n",
      "\n",
      "10. Paper ID: BJlaG0VFDH\n",
      "   Title: Decoupling Weight Regularization from Batch Size for Model Compression\n",
      "   Authors: Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, Yongkweon Jeon, Baeseong Park, Jeongin Yun, Gu-Yeon Wei...\n",
      "   Decision: Reject\n",
      "   Abstract: Conventionally, compression-aware training performs weight compression for every mini-batch to compute the impact of compression on the loss function. In this paper, in order to study when would be th...\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "As DataFrame (showing key columns):\n",
      "              id  year                                                                                   title                                                                                                            authors         decision        labels\n",
      "4129  HkgTTh4FDH  2020          Implicit Bias of Gradient Descent based Adversarial Training on Separable Data                                                                            Yan Li, Ethan X.Fang, Huan Xu, Tuo Zhao  Accept (Poster)  optimization\n",
      "3100  B1erJJrYPH  2020                             Optimizing Loss Landscape Connectivity via Neuron Alignment                              N. Joseph Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, Rongjie Lai           Reject  optimization\n",
      "3801  H1gEP6NFwr  2020                                        On the Tunability of Optimizers in Deep Learning                                         Prabhu Teja S*, Florian Mai*, Thijs Vogels, Martin Jaggi, Francois Fleuret           Reject  optimization\n",
      "4180  Hklcm0VYDS  2020             How noise affects the Hessian spectrum in overparameterized neural networks                                                                                          Mingwei Wei, David Schwab           Reject  optimization\n",
      "3633  BygJKn4tPr  2020                               Effective Mechanism to Mitigate Injuries During NFL Plays  Arraamuthan Arulanantham, Ahamed Arshad Ahamed Anzar, Gowshalini Rajalingam, Krusanth Ingran, Prasanna S. Haddela           Reject  optimization\n",
      "4108  HkerqCEtvS  2020                       Stochastic Gradient Methods with Block Diagonal Matrix Adaptation                                                                           Jihun Yun, Aurélie C. Lozano, Eunho Yang    Desk rejected  optimization\n",
      "3316  BJgnXpVYwS  2020  Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity                                                             Jingzhao Zhang, Tianxing He, Suvrit Sra, Ali Jadbabaie    Accept (Talk)  optimization\n",
      "5426  rkgqN1SYvr  2020        Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks                                                                            Wei Hu, Lechao Xiao, Jeffrey Pennington  Accept (Poster)  optimization\n",
      "3186  B1lxV6NFPH  2020      BANANAS: Bayesian Optimization with Neural Networks for Neural Architecture Search                                                                        Colin White, Willie Neiswanger, Yash Savani           Reject  optimization\n",
      "3360  BJlaG0VFDH  2020                  Decoupling Weight Regularization from Batch Size for Model Compression                 Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, Yongkweon Jeon, Baeseong Park, Jeongin Yun, Gu-Yeon Wei           Reject  optimization\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"10 RANDOM OPTIMIZATION PAPERS FROM 2020\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter for optimization label and year 2020\n",
    "optimization_2020 = df[(df['labels'] == 'optimization') & (df['year'] == 2020)]\n",
    "\n",
    "print(f\"\\nTotal optimization papers from 2020: {len(optimization_2020)}\")\n",
    "\n",
    "if len(optimization_2020) > 0:\n",
    "    # Sample 10 random papers (or all if less than 10)\n",
    "    n_samples = min(10, len(optimization_2020))\n",
    "    random_papers = optimization_2020.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "    print(f\"\\nShowing {n_samples} random papers:\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx, (i, row) in enumerate(random_papers.iterrows(), 1):\n",
    "        print(f\"\\n{idx}. Paper ID: {row.get('id', 'N/A')}\")\n",
    "        print(f\"   Title: {row.get('title', 'N/A')}\")\n",
    "        print(f\"   Authors: {row.get('authors', 'N/A')[:200]}...\")  # Truncate long author lists\n",
    "        print(f\"   Decision: {row.get('decision', 'N/A')}\")\n",
    "        if 'abstract' in row:\n",
    "            abstract_preview = str(row['abstract'])[:200] + \"...\" if len(str(row['abstract'])) > 200 else str(row['abstract'])\n",
    "            print(f\"   Abstract: {abstract_preview}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Also show as DataFrame for easier viewing\n",
    "    print(\"\\n\\nAs DataFrame (showing key columns):\")\n",
    "    display_cols = ['id', 'year', 'title', 'authors', 'decision', 'labels']\n",
    "    display_cols = [col for col in display_cols if col in random_papers.columns]\n",
    "    print(random_papers[display_cols].to_string())\n",
    "else:\n",
    "    print(\"\\nNo optimization papers found from 2020\")\n",
    "    print(f\"Available labels: {df['labels'].unique()[:10]}\")\n",
    "    print(f\"Available years: {sorted(df['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4bd1f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching reviews for 10 papers:\n",
      "  - HkgTTh4FDH\n",
      "  - B1erJJrYPH\n",
      "  - H1gEP6NFwr\n",
      "  - Hklcm0VYDS\n",
      "  - BygJKn4tPr\n",
      "  - HkerqCEtvS\n",
      "  - BJgnXpVYwS\n",
      "  - rkgqN1SYvr\n",
      "  - B1lxV6NFPH\n",
      "  - BJlaG0VFDH\n"
     ]
    }
   ],
   "source": [
    "# Get the paper IDs from the previous cell\n",
    "optimization_2020 = df[(df['labels'] == 'optimization') & (df['year'] == 2020)]\n",
    "random_papers = optimization_2020.sample(n=min(10, len(optimization_2020)), random_state=42)\n",
    "paper_ids = random_papers['id'].tolist()\n",
    "\n",
    "print(f\"Fetching reviews for {len(paper_ids)} papers:\")\n",
    "for pid in paper_ids:\n",
    "    print(f\"  - {pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b306c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching reviews using AutoReviewer approach...\n",
      "============================================================\n",
      "[1/10] Fetching reviews for HkgTTh4FDH...   Found 3 reviews ✓\n",
      "[2/10] Fetching reviews for B1erJJrYPH...   Found 3 reviews ✓\n",
      "[3/10] Fetching reviews for H1gEP6NFwr...   Found 2 reviews ✓\n",
      "[4/10] Fetching reviews for Hklcm0VYDS...   Found 3 reviews ✓\n",
      "[5/10] Fetching reviews for BygJKn4tPr...   Found 3 reviews ✓\n",
      "[6/10] Fetching reviews for HkerqCEtvS...   Found 4 notes but 0 reviews. Invitation types: {'Desk_Reject': 1, 'Official_Comment': 1, 'Public_Comment': 1, 'Desk_Rejected_Submission': 1} ✓\n",
      "[7/10] Fetching reviews for BJgnXpVYwS...   Found 3 reviews ✓\n",
      "[8/10] Fetching reviews for rkgqN1SYvr...   Found 3 reviews ✓\n",
      "[9/10] Fetching reviews for B1lxV6NFPH...   Found 3 reviews ✓\n",
      "[10/10] Fetching reviews for BJlaG0VFDH...   Found 3 reviews ✓\n",
      "\n",
      "============================================================\n",
      "Total reviews fetched: 26\n",
      "\n",
      "Reviews DataFrame shape: (26, 14)\n",
      "\n",
      "Columns: ['paper_id', 'review_id', 'review_type', 'rating', 'summary', 'strengths', 'weaknesses', 'questions', 'comment', 'review', 'content', 'cdate', 'writer', 'title']\n",
      "\n",
      "Reviews per paper:\n",
      "paper_id\n",
      "B1erJJrYPH    3\n",
      "B1lxV6NFPH    3\n",
      "BJgnXpVYwS    3\n",
      "BJlaG0VFDH    3\n",
      "BygJKn4tPr    3\n",
      "H1gEP6NFwr    2\n",
      "HkgTTh4FDH    3\n",
      "Hklcm0VYDS    3\n",
      "rkgqN1SYvr    3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import openreview\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_value_api2(field):\n",
    "    \"\"\"Extract value from API v2 nested dict format (used by AutoReviewer)\"\"\"\n",
    "    if isinstance(field, dict):\n",
    "        return field.get('value', field)\n",
    "    return field\n",
    "\n",
    "def content_dict_to_text(content_dict):\n",
    "    \"\"\"Extract text from content dictionary\"\"\"\n",
    "    main_keys = [\"title\", \"summary\", \"strengths\", \"weaknesses\", \"questions\", \"comment\", \"metareview\", \"rating\", \"review\"]\n",
    "    text = \"\"\n",
    "    for k in main_keys:\n",
    "        data = content_dict.get(k)\n",
    "        if data is None:\n",
    "            continue\n",
    "        # Handle both old format (direct value) and new format (dict with 'value')\n",
    "        if isinstance(data, dict) and 'value' in data:\n",
    "            text += f\"\\n{k}:\\n{data['value']}\\n\"\n",
    "        else:\n",
    "            text += f\"\\n{k}:\\n{str(data)}\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def get_reviews_for_paper(forum_id, year=2020):\n",
    "    \"\"\"\n",
    "    Fetch reviews for a specific paper using OpenReview client (AutoReviewer approach).\n",
    "    \n",
    "    For 2020 (API v1): Uses openreview.Client\n",
    "    For 2024+ (API v2): Uses openreview.api.OpenReviewClient\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize appropriate client based on year (following AutoReviewer approach)\n",
    "        if year < 2024:\n",
    "            client = openreview.Client(baseurl='https://api.openreview.net')\n",
    "        else:\n",
    "            client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')\n",
    "        \n",
    "        # Get all notes in the forum (submission + all replies)\n",
    "        # This is the proper way according to AutoReviewer\n",
    "        notes = client.get_notes(forum=forum_id)\n",
    "        \n",
    "        reviews = []\n",
    "        invitation_types = {}\n",
    "        \n",
    "        for note in notes:\n",
    "            # Get invitation - API v1 uses 'invitation' (string attribute), API v2 uses 'invitations' (list attribute)\n",
    "            if year < 2024:\n",
    "                invitation = getattr(note, 'invitation', '')\n",
    "                invitations = [invitation] if invitation else []\n",
    "            else:\n",
    "                invitations = getattr(note, 'invitations', [])\n",
    "                invitation = invitations[0] if invitations else ''\n",
    "            \n",
    "            if invitation:\n",
    "                inv_type = invitation.split(\"/\")[-1]\n",
    "                invitation_types[inv_type] = invitation_types.get(inv_type, 0) + 1\n",
    "            \n",
    "            # Check if it's an Official_Review (following AutoReviewer pattern)\n",
    "            is_review = False\n",
    "            if year < 2024:\n",
    "                is_review = 'Official_Review' in invitation\n",
    "            else:\n",
    "                is_review = any('Official_Review' in inv for inv in invitations) if invitations else False\n",
    "            \n",
    "            if is_review:\n",
    "                content = note.content if hasattr(note, 'content') else {}\n",
    "                \n",
    "                review_data = {\n",
    "                    'paper_id': forum_id,\n",
    "                    'review_id': note.id if hasattr(note, 'id') else '',\n",
    "                    'review_type': inv_type if invitation else '',\n",
    "                    'rating': None,\n",
    "                    'summary': None,\n",
    "                    'strengths': None,\n",
    "                    'weaknesses': None,\n",
    "                    'questions': None,\n",
    "                    'comment': None,\n",
    "                    'review': None,  # Full review text\n",
    "                    'content': None,\n",
    "                    'cdate': note.cdate if hasattr(note, 'cdate') else None,\n",
    "                    'writer': note.signatures[0] if hasattr(note, 'signatures') and note.signatures else None,\n",
    "                }\n",
    "                \n",
    "                # Extract fields - handle API v1 (direct) vs API v2 (nested) format\n",
    "                if year < 2024:\n",
    "                    # API v1: direct values\n",
    "                    review_data['rating'] = content.get('rating', '')\n",
    "                    review_data['review'] = content.get('review', '')\n",
    "                    for field in ['summary', 'strengths', 'weaknesses', 'questions', 'comment', 'title']:\n",
    "                        review_data[field] = content.get(field, '')\n",
    "                else:\n",
    "                    # API v2: nested {'value': ...} format\n",
    "                    review_data['rating'] = get_value_api2(content.get('rating', ''))\n",
    "                    review_data['review'] = get_value_api2(content.get('review', ''))\n",
    "                    for field in ['summary', 'strengths', 'weaknesses', 'questions', 'comment', 'title']:\n",
    "                        review_data[field] = get_value_api2(content.get(field, ''))\n",
    "                \n",
    "                # Get full content text for reference\n",
    "                review_data['content'] = content_dict_to_text(content)\n",
    "                \n",
    "                reviews.append(review_data)\n",
    "        \n",
    "        # Debug output\n",
    "        if len(reviews) == 0 and len(notes) > 0:\n",
    "            print(f\"  Found {len(notes)} notes but 0 reviews. Invitation types: {invitation_types}\", end=' ')\n",
    "        else:\n",
    "            print(f\"  Found {len(reviews)} reviews\", end=' ')\n",
    "        \n",
    "        return reviews\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)[:100]}\", end=' ')\n",
    "        return []\n",
    "\n",
    "# Fetch reviews for all papers\n",
    "all_reviews = []\n",
    "print(\"Fetching reviews using AutoReviewer approach...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, paper_id in enumerate(paper_ids, 1):\n",
    "    print(f\"[{i}/{len(paper_ids)}] Fetching reviews for {paper_id}...\", end=' ')\n",
    "    try:\n",
    "        reviews = get_reviews_for_paper(paper_id, year=2020)\n",
    "        all_reviews.extend(reviews)\n",
    "        print(f\"✓\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {str(e)[:50]}\")\n",
    "    time.sleep(0.5)  # Small delay to avoid rate limiting (like AutoReviewer)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"Total reviews fetched: {len(all_reviews)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "if all_reviews:\n",
    "    reviews_df = pd.DataFrame(all_reviews)\n",
    "    print(f\"\\nReviews DataFrame shape: {reviews_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(reviews_df.columns)}\")\n",
    "    print(f\"\\nReviews per paper:\")\n",
    "    print(reviews_df['paper_id'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"\\nNo reviews found!\")\n",
    "    reviews_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a64cddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REVIEWS DATAFRAME\n",
      "============================================================\n",
      "\n",
      "Total reviews: 26\n",
      "\n",
      "Reviews per paper:\n",
      "paper_id\n",
      "B1erJJrYPH    3\n",
      "B1lxV6NFPH    3\n",
      "BJgnXpVYwS    3\n",
      "BJlaG0VFDH    3\n",
      "BygJKn4tPr    3\n",
      "H1gEP6NFwr    2\n",
      "HkgTTh4FDH    3\n",
      "Hklcm0VYDS    3\n",
      "rkgqN1SYvr    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "First few reviews:\n",
      "     paper_id   review_id      review_type          rating summary strengths weaknesses questions comment                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         content          cdate                                           writer                     title\n",
      "0  HkgTTh4FDH  SkeJiVQ0Fr  Official_Review  6: Weak Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           **Contributions:**\\nThis paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\\n\\n**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded l2-norm, the classifier learned by GDAT converges to the maximum l2-norm margin classifier at the rate of O(1/√T), exponentially faster than the rate O (1/ log T ) obtained when training with only cleaned data Soudry et al. (2018) [3].\\n\\n**2) theoretical result for Lq norm:** When the adversarial perturbation during training has bounded l_q-norm with q > 1, with a proper choice of c, the gradient descent based adversarial training is directionally convergent to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum $l_2$-norm margin classifier under worst-case $l_q$-norm perturbation to the data.\\n\\nThe paper is well written and easy to understand. I haven’t checked the proofs, but I focused on readability and motivations. \\n\\nI have the following questions.\\n\\n**Theoretical questions:**\\n- Can you please explain the intuition of lemma 3.2 and 3.3 for convergence?\\n**Experiments questions:**\\nKeeping in mind that the core of the paper is about proving that GDAT enjoys the same implicit bias as GD and better convergence performance on clean data, I can’t help myself by asking you how these properties reflect in experiments with deep Neural Networks:\\n    - Can you please clarify your motivation for the reduction of the performance gap when the with of the hidden layer increases (Figure 2)? It is not clear what do you mean by ...“as network width increases, the margin on the samples outputted by the hidden layer also increases”...\\nMy personal interpretation of the results is that the inner maximization problem is much harder to solve and being approximated, the effect of the GDAT is much less prominent, meaning that PGD is not enough to benefit from adversarial training in this sense. What do you think about it?\\n    - Take away message of the paper: adversarial training accelerates convergence. Theoretical results are for the training empirical loss.\\nIs this also the case of the validation loss? You observe this behavior for the case of a single MLP on MNIST binary problem 2 vs 9 (appendix E) claiming that adversarial training improves generalization performance.\\nThis is not observed in the literature, it is actually the opposite [1, 2]: it exists a tension between robustness and performance on the clean test set. Why do you think you are not observing this? My guesses:\\n    - you are analysing a very simple binary problem\\n    - your model is very simple and comparable to a linear classifier, so your theoretical results hold.\\n\\n[1][1805.12152 Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)\\n[2][1810.12715 On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)\\n[3][1710.10345 The Implicit Bias of Gradient Descent on Separable Data](https://arxiv.org/abs/1710.10345)\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            title:\\nOfficial Blind Review #4\\n\\nrating:\\n6: Weak Accept\\n\\nreview:\\n**Contributions:**\\nThis paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\\n\\n**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded l2-norm, the classifier learned by GDAT converges to the maximum l2-norm margin classifier at the rate of O(1/√T), exponentially faster than the rate O (1/ log T ) obtained when training with only cleaned data Soudry et al. (2018) [3].\\n\\n**2) theoretical result for Lq norm:** When the adversarial perturbation during training has bounded l_q-norm with q > 1, with a proper choice of c, the gradient descent based adversarial training is directionally convergent to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum $l_2$-norm margin classifier under worst-case $l_q$-norm perturbation to the data.\\n\\nThe paper is well written and easy to understand. I haven’t checked the proofs, but I focused on readability and motivations. \\n\\nI have the following questions.\\n\\n**Theoretical questions:**\\n- Can you please explain the intuition of lemma 3.2 and 3.3 for convergence?\\n**Experiments questions:**\\nKeeping in mind that the core of the paper is about proving that GDAT enjoys the same implicit bias as GD and better convergence performance on clean data, I can’t help myself by asking you how these properties reflect in experiments with deep Neural Networks:\\n    - Can you please clarify your motivation for the reduction of the performance gap when the with of the hidden layer increases (Figure 2)? It is not clear what do you mean by ...“as network width increases, the margin on the samples outputted by the hidden layer also increases”...\\nMy personal interpretation of the results is that the inner maximization problem is much harder to solve and being approximated, the effect of the GDAT is much less prominent, meaning that PGD is not enough to benefit from adversarial training in this sense. What do you think about it?\\n    - Take away message of the paper: adversarial training accelerates convergence. Theoretical results are for the training empirical loss.\\nIs this also the case of the validation loss? You observe this behavior for the case of a single MLP on MNIST binary problem 2 vs 9 (appendix E) claiming that adversarial training improves generalization performance.\\nThis is not observed in the literature, it is actually the opposite [1, 2]: it exists a tension between robustness and performance on the clean test set. Why do you think you are not observing this? My guesses:\\n    - you are analysing a very simple binary problem\\n    - your model is very simple and comparable to a linear classifier, so your theoretical results hold.\\n\\n[1][1805.12152 Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)\\n[2][1810.12715 On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)\\n[3][1710.10345 The Implicit Bias of Gradient Descent on Separable Data](https://arxiv.org/abs/1710.10345)  1571857559418   ICLR.cc/2020/Conference/Paper244/AnonReviewer4  Official Blind Review #4\n",
      "1  HkgTTh4FDH  H1eIh4uhYB  Official_Review       8: Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. Both l2 and general lq perturbations are studied.\\n\\nFor l2, the authors show that for small enough perturbations, the algorithm converges in direction to the max l2 margin solution, with faster convergence rates compared to standard gradient descent, both for the clean loss and the parameter direction (O(1/sqrt(T)) instead of O(1/log(T))).\\nFor lq, convergence to a different max-margin direction is shown, given according to a mixture of the l2 and lp norms, though the parameter convergence is slower.\\nThese results are further illustrated by numerical experiments.\\n\\nThe topic of the paper is novel and interesting, in particular the finding that adversarial training can lead to benefits in terms of optimization in addition to robustness, as well as the characterization of the inductive bias obtained when using lq perturbations in adversarial training (i.e. a mixture of lp and l2 norms, rather than just lp). Granted, the setting of linear models is a bit limited, but it provides a first step for more realistic models. The paper is also well written, and also has a nice numerical validation of the results. Overall, I am in favor of acceptance.\\n\\nHere are some questions/comments that could be further discussed:\\n* for l2, while the obtained rates for parameter convergence are better in their dependence on T, they depend on a data-dependent quantity alpha in contrast to standard GD, suggesting that the rate could be worse for small alpha: is there a trade-off here? would standard GD be preferable if alpha is small, or could the current rate (7) automatically adapt to such settings?\\n\\n* in Theorem 3.4, how does the choice of c come into play? can you obtain better rates by optimizing it (ideally this should happen when q=2)? Regarding the comment on tending to the lq-norm margin for c -> gamma_q, is this at the cost of poorer convergence?\\n\\n* for lq perturbations, while I agree that the studied algorithm is interesting since that's what's used in practice for deep networks, I do wonder if in this specific setting you could get better convergence (and with the right norm lp instead of the mixture) by optimizing using the appropriate geometry, e.g. with mirror descent.\\n\\n* the analyzed algorithm considers optimal perturbations -- do you have a sense of how robust the theory is to errors in the inner optimization?\\n\\n\\nminor comments/typos:\\n* second bullet in 'main contributions': 'mixed-norm margin' could be further explained, or at least point to the definition. The terminology is also confusing as it sounds like a matrix mixed-norm\\n* 'polyhedral cone ...': bar{u} should be u?\\n* after Theorem 3.2 'not an issue': not so clear, this should be discussed further (see comment above)\\n* section 3.2, \"defer discussion for 1, infty\": you could provide a flavor of the results for these cases in the main text, given their prominence in practice\\n* figure 1(b): for the direction plot, is it actually u_2? how should this plot be interpreted given that the two curves converge to a different direction?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              title:\\nOfficial Blind Review #3\\n\\nrating:\\n8: Accept\\n\\nreview:\\nThe paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. Both l2 and general lq perturbations are studied.\\n\\nFor l2, the authors show that for small enough perturbations, the algorithm converges in direction to the max l2 margin solution, with faster convergence rates compared to standard gradient descent, both for the clean loss and the parameter direction (O(1/sqrt(T)) instead of O(1/log(T))).\\nFor lq, convergence to a different max-margin direction is shown, given according to a mixture of the l2 and lp norms, though the parameter convergence is slower.\\nThese results are further illustrated by numerical experiments.\\n\\nThe topic of the paper is novel and interesting, in particular the finding that adversarial training can lead to benefits in terms of optimization in addition to robustness, as well as the characterization of the inductive bias obtained when using lq perturbations in adversarial training (i.e. a mixture of lp and l2 norms, rather than just lp). Granted, the setting of linear models is a bit limited, but it provides a first step for more realistic models. The paper is also well written, and also has a nice numerical validation of the results. Overall, I am in favor of acceptance.\\n\\nHere are some questions/comments that could be further discussed:\\n* for l2, while the obtained rates for parameter convergence are better in their dependence on T, they depend on a data-dependent quantity alpha in contrast to standard GD, suggesting that the rate could be worse for small alpha: is there a trade-off here? would standard GD be preferable if alpha is small, or could the current rate (7) automatically adapt to such settings?\\n\\n* in Theorem 3.4, how does the choice of c come into play? can you obtain better rates by optimizing it (ideally this should happen when q=2)? Regarding the comment on tending to the lq-norm margin for c -> gamma_q, is this at the cost of poorer convergence?\\n\\n* for lq perturbations, while I agree that the studied algorithm is interesting since that's what's used in practice for deep networks, I do wonder if in this specific setting you could get better convergence (and with the right norm lp instead of the mixture) by optimizing using the appropriate geometry, e.g. with mirror descent.\\n\\n* the analyzed algorithm considers optimal perturbations -- do you have a sense of how robust the theory is to errors in the inner optimization?\\n\\n\\nminor comments/typos:\\n* second bullet in 'main contributions': 'mixed-norm margin' could be further explained, or at least point to the definition. The terminology is also confusing as it sounds like a matrix mixed-norm\\n* 'polyhedral cone ...': bar{u} should be u?\\n* after Theorem 3.2 'not an issue': not so clear, this should be discussed further (see comment above)\\n* section 3.2, \"defer discussion for 1, infty\": you could provide a flavor of the results for these cases in the main text, given their prominence in practice\\n* figure 1(b): for the direction plot, is it actually u_2? how should this plot be interpreted given that the two curves converge to a different direction?  1571746990294   ICLR.cc/2020/Conference/Paper244/AnonReviewer3  Official Blind Review #3\n",
      "2  HkgTTh4FDH  ryePl_e5Kr  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The paper applies theory on implicit bias of gradient descent to an adversarial training toy example. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness. The theoretical results are complemented with experiments on linear classifiers and small neural networks.\\n\\nThe key technical contributions look mostly like an application of the existing theory on implicit bias of gradient descent, thus I would rate the originality of this work as moderate. From the point of view of adversarial machine learning, I don't consider the results to be significant. In particular, there is a large discrepancy between the theoretical results on the toy example and the empirical convergence behaviour of adversarial training of neural networks (compare Figure 2 and 3). In particular, the theoretically derived exponential speed-up of adversarial training for the toy example is not reflected by empirical observations for practically relevant adversarial training tasks (e.g. consider the convergence behaviours reported by Goodfellow et al. (2014) or Madry et al. (2017)). Thus, the theory doesn't seem to be generally applicable beyond the linear toy model. A clear discussion of its limitations is missing.\\n\\nOverall, while this may be an interesting extension of the existing body of literature on the implicit bias of gradient descent, I find that from an adversarial machine learning perspective this paper does not meet its objective to derive generally applicable insights on how adversarial training works.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    title:\\nOfficial Blind Review #1\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThe paper applies theory on implicit bias of gradient descent to an adversarial training toy example. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness. The theoretical results are complemented with experiments on linear classifiers and small neural networks.\\n\\nThe key technical contributions look mostly like an application of the existing theory on implicit bias of gradient descent, thus I would rate the originality of this work as moderate. From the point of view of adversarial machine learning, I don't consider the results to be significant. In particular, there is a large discrepancy between the theoretical results on the toy example and the empirical convergence behaviour of adversarial training of neural networks (compare Figure 2 and 3). In particular, the theoretically derived exponential speed-up of adversarial training for the toy example is not reflected by empirical observations for practically relevant adversarial training tasks (e.g. consider the convergence behaviours reported by Goodfellow et al. (2014) or Madry et al. (2017)). Thus, the theory doesn't seem to be generally applicable beyond the linear toy model. A clear discussion of its limitations is missing.\\n\\nOverall, while this may be an interesting extension of the existing body of literature on the implicit bias of gradient descent, I find that from an adversarial machine learning perspective this paper does not meet its objective to derive generally applicable insights on how adversarial training works.  1571583982950   ICLR.cc/2020/Conference/Paper244/AnonReviewer1  Official Blind Review #1\n",
      "3  B1erJJrYPH  BJxTPBxM9S  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This paper combines neuron alignment with mode connectivity. Specifically, it applies paths neuron alignment to the calculation of mode-connecting and empirical results show that alignment helps in finding better curves.\\nCombining neuron alignment with mode connectivity seems to be a good idea, but the message the authors want to convey is somewhat vague. Some key details are not presented clearly, and some contents seem to be irrelevant. The following are some detailed comments and questions:\\n1. One main contribution of this paper is the observation that the observation that alignment helps in finding better curves. An observation is excellent if it brings significant performance improvements in practice, or if it brings deep insights in the understanding of the field. However, for the former, the improvement in the performance is not that much; for the latter, there is hardly any insight conveyed by this paper. Therefore, this observation itself is not strong enough.\\n2. One contribution of this paper is applying proximal alternating minimization (PAM) when optimizing the parameters and proving its convergence. Nonetheless, PAM is only used in one model (TinyTen) and does not bring any improvement in the performance. It seems that there is no point in applying PAM and the contents related to PAM are all somewhat irrelevant.\\n3. Usually sufficient details help in good understandings, but in this paper, some key details are unfortunately missing. For example, in Algorithm 2, details on the optimization step is not clear: what is the optimization method the authors use other than PAM? Also, no comments are addressed on Figure 7 to Figure 10, either in the main body or in the appendix. I would like to see more explanations details. If the source code is provided, it will be better.\\nIn sum, the idea seems to be interesting, but the overall quality of the paper is still yet to be improved, and some key details need to be addressed more clearly before it can be accepted as a qualified submission.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           title:\\nOfficial Blind Review #2\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThis paper combines neuron alignment with mode connectivity. Specifically, it applies paths neuron alignment to the calculation of mode-connecting and empirical results show that alignment helps in finding better curves.\\nCombining neuron alignment with mode connectivity seems to be a good idea, but the message the authors want to convey is somewhat vague. Some key details are not presented clearly, and some contents seem to be irrelevant. The following are some detailed comments and questions:\\n1. One main contribution of this paper is the observation that the observation that alignment helps in finding better curves. An observation is excellent if it brings significant performance improvements in practice, or if it brings deep insights in the understanding of the field. However, for the former, the improvement in the performance is not that much; for the latter, there is hardly any insight conveyed by this paper. Therefore, this observation itself is not strong enough.\\n2. One contribution of this paper is applying proximal alternating minimization (PAM) when optimizing the parameters and proving its convergence. Nonetheless, PAM is only used in one model (TinyTen) and does not bring any improvement in the performance. It seems that there is no point in applying PAM and the contents related to PAM are all somewhat irrelevant.\\n3. Usually sufficient details help in good understandings, but in this paper, some key details are unfortunately missing. For example, in Algorithm 2, details on the optimization step is not clear: what is the optimization method the authors use other than PAM? Also, no comments are addressed on Figure 7 to Figure 10, either in the main body or in the appendix. I would like to see more explanations details. If the source code is provided, it will be better.\\nIn sum, the idea seems to be interesting, but the overall quality of the paper is still yet to be improved, and some key details need to be addressed more clearly before it can be accepted as a qualified submission.  1572107621232  ICLR.cc/2020/Conference/Paper1466/AnonReviewer2  Official Blind Review #2\n",
      "4  B1erJJrYPH  SylrU09ptS  Official_Review  6: Weak Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The paper investigates the connection between symmetries in the neural network architectures and the loss landscape of the corresponding neural networks. In the previous works, there was shown that the two local minima of a neural network can be connected by a curve with the low validation/train loss along the curve. Despite the loss on the curve being close to the loss at the end points, there are segments of the curve on which the loss in higher than loss at the local minima. To overcome this problem, the authors proposed two-step procedure: 1. Align weights between two neural networks 2. Launch the path finding algorithm between aligned weights. In other words, the authors proposed to connect not original local minima but local minima that parametrize the same functions as the original ones, but have a different order of neurons. The authors also proposed PAM algorithm where they iteratively apply path finding algorithm and weights alignment algorithm.\\n\\nNovelty and significance. The idea to combine the symmetrization of NN architectures with path finding algorithms is new to the best of my knowledge. Experimentally, the authors showed that ensembling the midpoint and endpoints of the curve found via path finding  algorithm coupled with the neural alignment algorithm delivers a better result than simple averaging of three independent networks. This is a new and significant result, since before the ensembling of points along the curve had the same quality as the ensemble of three independent networks or marginally better.  \\nThe weak side of the paper is the PAM algorithm that occupies a significant part of the paper and does not deliver a significantly better result than the simple application of the neural alignment procedure before launching the path finding algorithm.\\n\\nClarity. Overall, the related work section contains all relevant references to the previous works to the best of my knowledge. The paper is well written, excluding the section Neuron Alignment that lacks notation description.  \\n\\nThe paper contains several typos and inaccuracies:\\n1. “However, We find its empirical performance is similar to standard curve finding, and we advocate the latter for practical use due to computational efficiency.” The word “We” should start with the lowercase letter.\\n2. In the sentence “The first question to address is how to effectively deal with the constraints in equation 6” the index i should be replaced with the index l.\\n3. Notation Π|Kl| introduced after equation 5.\\n4. In the section describing neuron alignment  algorithm Lie et al. [1] used a different notation. So I would recommend to further extend this section and add all necessary notations. Also, I would recommend to add a direct link to the paper where the problem is described in matrix form.\\n5. In the Algorithm 1 “Initialize P θ2 := [Wˆ 2 1 ,Wˆ 2 2 , ...,Wˆ 2 L ] as [W2 1 ,W2 2 , ...,W2 L ] for k ∈ {1, 2, ..., L − 1};”  k is not used anywhere in notation.\\n6. In Figure 3, “model 2 aligned ” sing is out of the plot box for ResNet-32 and VGG-16 architectures.\\n7. The appendix contains the sketch of the proof that is quite difficult to follow. I would recommend giving all necessary definitions as it is done in the [2] and extend the proof. \\n\\nOverall, it is quite an interesting paper but it contains some drawbacks.\\n[1] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E Hopcroft.  Convergent learning: Do different neural networks learn the same representations?  In ICLR, 2016\\n\\n[2]  H́edy Attouch, J́erˆome Bolte, Patrick Redont, and Antoine Soubeyran.  Proximal alternating minimization and projection methods for nonconvex problems:  An approach based on the kurdyka-łojasiewicz inequality.Mathematics of Operations Research, 35(2):438–457, 2010\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         title:\\nOfficial Blind Review #3\\n\\nrating:\\n6: Weak Accept\\n\\nreview:\\nThe paper investigates the connection between symmetries in the neural network architectures and the loss landscape of the corresponding neural networks. In the previous works, there was shown that the two local minima of a neural network can be connected by a curve with the low validation/train loss along the curve. Despite the loss on the curve being close to the loss at the end points, there are segments of the curve on which the loss in higher than loss at the local minima. To overcome this problem, the authors proposed two-step procedure: 1. Align weights between two neural networks 2. Launch the path finding algorithm between aligned weights. In other words, the authors proposed to connect not original local minima but local minima that parametrize the same functions as the original ones, but have a different order of neurons. The authors also proposed PAM algorithm where they iteratively apply path finding algorithm and weights alignment algorithm.\\n\\nNovelty and significance. The idea to combine the symmetrization of NN architectures with path finding algorithms is new to the best of my knowledge. Experimentally, the authors showed that ensembling the midpoint and endpoints of the curve found via path finding  algorithm coupled with the neural alignment algorithm delivers a better result than simple averaging of three independent networks. This is a new and significant result, since before the ensembling of points along the curve had the same quality as the ensemble of three independent networks or marginally better.  \\nThe weak side of the paper is the PAM algorithm that occupies a significant part of the paper and does not deliver a significantly better result than the simple application of the neural alignment procedure before launching the path finding algorithm.\\n\\nClarity. Overall, the related work section contains all relevant references to the previous works to the best of my knowledge. The paper is well written, excluding the section Neuron Alignment that lacks notation description.  \\n\\nThe paper contains several typos and inaccuracies:\\n1. “However, We find its empirical performance is similar to standard curve finding, and we advocate the latter for practical use due to computational efficiency.” The word “We” should start with the lowercase letter.\\n2. In the sentence “The first question to address is how to effectively deal with the constraints in equation 6” the index i should be replaced with the index l.\\n3. Notation Π|Kl| introduced after equation 5.\\n4. In the section describing neuron alignment  algorithm Lie et al. [1] used a different notation. So I would recommend to further extend this section and add all necessary notations. Also, I would recommend to add a direct link to the paper where the problem is described in matrix form.\\n5. In the Algorithm 1 “Initialize P θ2 := [Wˆ 2 1 ,Wˆ 2 2 , ...,Wˆ 2 L ] as [W2 1 ,W2 2 , ...,W2 L ] for k ∈ {1, 2, ..., L − 1};”  k is not used anywhere in notation.\\n6. In Figure 3, “model 2 aligned ” sing is out of the plot box for ResNet-32 and VGG-16 architectures.\\n7. The appendix contains the sketch of the proof that is quite difficult to follow. I would recommend giving all necessary definitions as it is done in the [2] and extend the proof. \\n\\nOverall, it is quite an interesting paper but it contains some drawbacks.\\n[1] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E Hopcroft.  Convergent learning: Do different neural networks learn the same representations?  In ICLR, 2016\\n\\n[2]  H́edy Attouch, J́erˆome Bolte, Patrick Redont, and Antoine Soubeyran.  Proximal alternating minimization and projection methods for nonconvex problems:  An approach based on the kurdyka-łojasiewicz inequality.Mathematics of Operations Research, 35(2):438–457, 2010  1571823181024  ICLR.cc/2020/Conference/Paper1466/AnonReviewer3  Official Blind Review #3\n",
      "5  B1erJJrYPH  SJeZKdchKH  Official_Review       1: Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Given two parameters theta_1 and theta_2 of the same architecture, the authors propose to learn a minimal loss curve between theta_1 and P theta_2, where P is a permutation matrix yielding another equivalent parameterization of the same network. The authors show that either by initializing P with neuron alignment or by optimizing P jointly with the curve, one can find a parameter-space with better accuracy and loss than by just learning a curve between theta_1 and theta_2. The authors also show that initializing P is sufficient to obtain these gains, avoiding the complexity of also optimizing for P. Furthermore, they show that ensembles across models from these curves have a very mild gain in accuracy to those of non-aligned curves.\\n\\nThe main qualm I have about this paper is about the significance of the contributions and the motivation.\\nAt the core, the authors propose to find a curve between theta_1 and P theta_2 where P comes from aligning theta_1 and theta_2 as in (Li et al. 2016.). This is lacking almost any motivation, or discussion on what problem they are trying to solve. Are they trying to find ensembles with lower error? If that is the case, well the results are evidence of a negative result in this respect, which is okay but given how ad-hoc and poorly motivated the method is to that objective it's not much of a contribution. Are they trying to better understand theoretically the loss landscape of neural networks? I don't think there's any theoretical gain in that regard from this paper either. They show that the curves between aligned networks are better, but they don't show how this relates to anything else in the published literature or open questions in the theoretical deep learning field.\\n\\nRegarding contribution 2, PAM is in the end shown to not converge to better curves than simply initializing with alignment. Also, doesn't the convergence result to a critical point also apply for standard descent methods? The convergence theorem doesn't seem to be much of a contribution in my opinion, either to the optimization or the deep learning community.\\n\\nRegarding contribution 3, I agree that better curves can be learned faster, but why is this a meaningful contribution? What problem that the deep learning or the theoretical machine learning cares about does this help solve?\\n\\nRegarding contribution 4, as the authors themselves admit, the improvement is very dim in comparison to non-aligned curves, and comparisons to other ensemble methods are not present.\\n\\nI want to acknowledge that while the motivation and contributions are in my opinion dim, I find the experimental methodology of this paper very solid. All the claims are to my extent very well validated with experiments, and the experiments are in general well designed to validate those claims. My problem is sadly not with the validity of the claims but with their significance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          title:\\nOfficial Blind Review #1\\n\\nrating:\\n1: Reject\\n\\nreview:\\nGiven two parameters theta_1 and theta_2 of the same architecture, the authors propose to learn a minimal loss curve between theta_1 and P theta_2, where P is a permutation matrix yielding another equivalent parameterization of the same network. The authors show that either by initializing P with neuron alignment or by optimizing P jointly with the curve, one can find a parameter-space with better accuracy and loss than by just learning a curve between theta_1 and theta_2. The authors also show that initializing P is sufficient to obtain these gains, avoiding the complexity of also optimizing for P. Furthermore, they show that ensembles across models from these curves have a very mild gain in accuracy to those of non-aligned curves.\\n\\nThe main qualm I have about this paper is about the significance of the contributions and the motivation.\\nAt the core, the authors propose to find a curve between theta_1 and P theta_2 where P comes from aligning theta_1 and theta_2 as in (Li et al. 2016.). This is lacking almost any motivation, or discussion on what problem they are trying to solve. Are they trying to find ensembles with lower error? If that is the case, well the results are evidence of a negative result in this respect, which is okay but given how ad-hoc and poorly motivated the method is to that objective it's not much of a contribution. Are they trying to better understand theoretically the loss landscape of neural networks? I don't think there's any theoretical gain in that regard from this paper either. They show that the curves between aligned networks are better, but they don't show how this relates to anything else in the published literature or open questions in the theoretical deep learning field.\\n\\nRegarding contribution 2, PAM is in the end shown to not converge to better curves than simply initializing with alignment. Also, doesn't the convergence result to a critical point also apply for standard descent methods? The convergence theorem doesn't seem to be much of a contribution in my opinion, either to the optimization or the deep learning community.\\n\\nRegarding contribution 3, I agree that better curves can be learned faster, but why is this a meaningful contribution? What problem that the deep learning or the theoretical machine learning cares about does this help solve?\\n\\nRegarding contribution 4, as the authors themselves admit, the improvement is very dim in comparison to non-aligned curves, and comparisons to other ensemble methods are not present.\\n\\nI want to acknowledge that while the motivation and contributions are in my opinion dim, I find the experimental methodology of this paper very solid. All the claims are to my extent very well validated with experiments, and the experiments are in general well designed to validate those claims. My problem is sadly not with the validity of the claims but with their significance.  1571756153378  ICLR.cc/2020/Conference/Paper1466/AnonReviewer1  Official Blind Review #1\n",
      "6  H1gEP6NFwr  BylSI7T6qr  Official_Review  3: Weak Reject                                                 This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. The tunability of the optimizer is a weighted sum of best performance at a given budget. The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer’s learning rate is likely to be a very good choice; it doesn’t guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations.\\n\\nComments:\\n\\nThe paper is easy to follow. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons:\\n\\nIn section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of “sharpness” of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions. Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails.\\n\\nIn addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. \\n\\nThe definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable. \\n\\nThe authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO. My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. \\n\\nMy major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM. Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.\\n\\n\\nThe author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained. Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?\\n\\n[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451\\n\\n[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489\\n\\n[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350\\n\\n[4] Rethinking the Hyperparameters for Fine-tuning\\n https://openreview.net/forum?id=B1g8VkHFPH\\n  title:\\nOfficial Blind Review #2\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThis paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. The tunability of the optimizer is a weighted sum of best performance at a given budget. The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer’s learning rate is likely to be a very good choice; it doesn’t guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations.\\n\\nComments:\\n\\nThe paper is easy to follow. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons:\\n\\nIn section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of “sharpness” of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions. Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails.\\n\\nIn addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. \\n\\nThe definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable. \\n\\nThe authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO. My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. \\n\\nMy major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM. Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.\\n\\n\\nThe author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained. Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?\\n\\n[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451\\n\\n[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489\\n\\n[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350\\n\\n[4] Rethinking the Hyperparameters for Fine-tuning\\n https://openreview.net/forum?id=B1g8VkHFPH  1572881228757   ICLR.cc/2020/Conference/Paper593/AnonReviewer2  Official Blind Review #2\n",
      "7  H1gEP6NFwr  SJx0zByAYr  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The main contributions of the submission are:\\n\\n1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).\\n2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer. This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned. The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are \"hyper-parameters\" of the metric itself. They use K=100 and suggest 3 possible choices of weights.\\n\\nThe paper appears to treat 2. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset. Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics. Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).\\n\\nA stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past. Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work. Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.\\n\\nThey also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.\\n\\nI enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.\\n\\nOther comments/notes:\\n* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials. I think it would be worth discussing this more.\\n* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)\\n* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    title:\\nOfficial Blind Review #1\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThe main contributions of the submission are:\\n\\n1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).\\n2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer. This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned. The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are \"hyper-parameters\" of the metric itself. They use K=100 and suggest 3 possible choices of weights.\\n\\nThe paper appears to treat 2. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset. Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics. Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).\\n\\nA stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past. Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work. Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.\\n\\nThey also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.\\n\\nI enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.\\n\\nOther comments/notes:\\n* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials. I think it would be worth discussing this more.\\n* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)\\n* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases.  1571841301558   ICLR.cc/2020/Conference/Paper593/AnonReviewer1  Official Blind Review #1\n",
      "8  Hklcm0VYDS  BJxOztS89r  Official_Review  6: Weak Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      This paper studies the behavior of SGD after it has reached a steady state and settled in a minimal valley. The authors show that even if SGD has reached a minimal valley, the noisy updates provided by a minibatch of samples result in the trace of the Hessians reducing with further SGD iterations. The authors also show that by changing the type of noise that is added to GD, one can get different functions of the Hessian to reduce with SGD iterations. The theoretical conclusions are then verified empirically with synthetic examples and real deep learning examples\\n\\nIn my opinion this is an interesting paper and research direction that helps us understand how SGD biases solutions towards \"flatter\" minima as measured by the trace norm of the Hessian of the loss function. I have a few concerns though:\\n\\n1. Is the steady state assumption valid for neural network training? Especially when training on exponential losses (like cross entropy) which drives the parameters towards large norm solutions? This seems to be an important yet unsupported assumption in the analysis.\\n\\n2. Does the Hessian-noise covariance alignment exist for squared loss functions as well? Is this specific to log-likelihood models?\\n\\n3. In the experiments to delineate how different types of noise can result in regularization of different quantities, is there a reason only a synthetic example is used? Can this be replicated for deep networks, or are the two quantities (trace vs determinant) closely related? It would be nice to see how this behavior extends to deep networks.\\n\\nAdditional questions/comments:\\n1. While overparameterization seems to be important to the analysis (the parameters move in the degenerate subspace but not in the non-degenerate one), is there anyway one can amend this framework to analyze different levels of overparameterization? Is there any difference in the rates of decrease in the trace norms for highly overparameterized models vs lightly overparameterized ones?\\n\\n2. This paper talks about how SGD has a bias towards flatter minima. Is there a reason to prefer flatter solutions over sharper ones to get better generalization? Can one prove this connection?\\n\\n3. In the Densenet experiments in Figure 4a, it seems as though this relationship between flatness and generalization might not hold? There are points along the optimization trajectory where the validation loss seems to be better than at the last few points along the optimization path. However at the former set of points the trace norm of the hessian is larger than at the latter points. Is this from a stray experiment, or are the plots averaged over a number of different runs? Please add these details as well as other details such as learning rates, criteria to decide when steady state was reached, whether or not batch norm was used, etc.\\n\\n4. The Fluctuation-Dissipation Relations need to be explained more clearly. For people unfamiliar with statistical physics (and Yaida 2019) the notation and formulation is not immediately clear and that makes the paper harder to read.\\n\\nOverall I believe this paper explains an important phenomenon. I am willing to update my score if my concerns are addressed/if there is any misunderstanding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     title:\\nOfficial Blind Review #2\\n\\nrating:\\n6: Weak Accept\\n\\nreview:\\nThis paper studies the behavior of SGD after it has reached a steady state and settled in a minimal valley. The authors show that even if SGD has reached a minimal valley, the noisy updates provided by a minibatch of samples result in the trace of the Hessians reducing with further SGD iterations. The authors also show that by changing the type of noise that is added to GD, one can get different functions of the Hessian to reduce with SGD iterations. The theoretical conclusions are then verified empirically with synthetic examples and real deep learning examples\\n\\nIn my opinion this is an interesting paper and research direction that helps us understand how SGD biases solutions towards \"flatter\" minima as measured by the trace norm of the Hessian of the loss function. I have a few concerns though:\\n\\n1. Is the steady state assumption valid for neural network training? Especially when training on exponential losses (like cross entropy) which drives the parameters towards large norm solutions? This seems to be an important yet unsupported assumption in the analysis.\\n\\n2. Does the Hessian-noise covariance alignment exist for squared loss functions as well? Is this specific to log-likelihood models?\\n\\n3. In the experiments to delineate how different types of noise can result in regularization of different quantities, is there a reason only a synthetic example is used? Can this be replicated for deep networks, or are the two quantities (trace vs determinant) closely related? It would be nice to see how this behavior extends to deep networks.\\n\\nAdditional questions/comments:\\n1. While overparameterization seems to be important to the analysis (the parameters move in the degenerate subspace but not in the non-degenerate one), is there anyway one can amend this framework to analyze different levels of overparameterization? Is there any difference in the rates of decrease in the trace norms for highly overparameterized models vs lightly overparameterized ones?\\n\\n2. This paper talks about how SGD has a bias towards flatter minima. Is there a reason to prefer flatter solutions over sharper ones to get better generalization? Can one prove this connection?\\n\\n3. In the Densenet experiments in Figure 4a, it seems as though this relationship between flatness and generalization might not hold? There are points along the optimization trajectory where the validation loss seems to be better than at the last few points along the optimization path. However at the former set of points the trace norm of the hessian is larger than at the latter points. Is this from a stray experiment, or are the plots averaged over a number of different runs? Please add these details as well as other details such as learning rates, criteria to decide when steady state was reached, whether or not batch norm was used, etc.\\n\\n4. The Fluctuation-Dissipation Relations need to be explained more clearly. For people unfamiliar with statistical physics (and Yaida 2019) the notation and formulation is not immediately clear and that makes the paper harder to read.\\n\\nOverall I believe this paper explains an important phenomenon. I am willing to update my score if my concerns are addressed/if there is any misunderstanding.  1572391183537  ICLR.cc/2020/Conference/Paper1047/AnonReviewer2  Official Blind Review #2\n",
      "9  Hklcm0VYDS  H1lum-xpYr  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This paper proves that under certain conditions, SGD on average decreases the\\ntrace of the Hessian of the loss. The paper is overall clear and the topic addressed in the paper is relevant in machine learning but I don’t think this paper is ready for publication. There are numerous assumptions that are made in the paper, but not properly stated or backed up. In my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.\\n\\nLemma 1\\nThis is a corollary of Morse lemma, which is also used in Dauphin et al: https://arxiv.org/pdf/1406.2572.pdf (see Equation 1). I don’t see why you would need to re-derive it in your paper and most importantly this should be clearly stated in your paper.\\n\\nPage 2: existence stationary state?\\nWhat are the conditions for the existence of a stationary state? Is bounded noise sufficient?\\n\\nEquation 4 and local approximation\\nRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss-Newton decomposition), the discussion is not precise. The authors claim “Empirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum“\\nBut clearly from the formula itself, one needs to differentiate between minima with low and high function values. For local minima that are high in the energy landscape, the second term might have a large function value. If of course all **depends on the size of the approximation region**. This is extremely important and it is not properly characterized in the paper. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated. Note that similar types of analyses are usually done in dynamical systems where the behavior of a system is linearized around a critical point. In some cases, one can however characterize the size of a basin of attraction, see e.g. the book Nonlinear Systems (3rd Edition): Hassan K. Khalil.\\n\\nAssumption on timescale separation\\nThis section makes numerous claims that are not properly backed up.\\n1) “This assumption is because there is a timescale separation between\\nthe dynamics of \\theta_bar, which relax quickly and the dynamics of \\theta_hat, which evolve much more slowly as the minimal valley is traversed.“\\nAre you claiming the absolute value of the eigenvalue of the non-degenerate space are much smaller than the degenerate space? Why would that be so?\\n2) Ornstein-Uhlenbeck: OU processes require the noise to be Brownian motion. This assumption needs to be clearly stated. Note that there is actually evidence that the noise of SGD is heavy-tail:\\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. \"A tail-index analysis of stochastic gradient noise in deep neural networks.\" arXiv preprint arXiv:1901.06053 (2019).\\n\\nTake away\\nI am unsure what the added value of this paper is. Second-order methods can already be shown to decrease the maximum eigenvalue, see e.g. convergence results derived in Cubic regularization of Newton method and its global performance by Nesterov and Polyak. These methods have also been analyzed in stochastic settings where similar convergence results hold as well. What particular insight do we gain from the results of Theorem 2? The authors claim this could “potentially improve generalization” but this is not justified and no reference is cited.\\n\\n“This indicates that the trace itself is not sufficient to describe generalization (Neyshabur et al., 2017).“\\nI do not see what aspect of Neyshabur et al. justifies your claim, please explain.\\n\\nExperiments\\nAll the experiments performed in the paper are on very small models. Given the rather strong assumptions made in the paper, I feel that the paper should provide stronger empirical evidence to back up their claims.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              title:\\nOfficial Blind Review #1\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThis paper proves that under certain conditions, SGD on average decreases the\\ntrace of the Hessian of the loss. The paper is overall clear and the topic addressed in the paper is relevant in machine learning but I don’t think this paper is ready for publication. There are numerous assumptions that are made in the paper, but not properly stated or backed up. In my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.\\n\\nLemma 1\\nThis is a corollary of Morse lemma, which is also used in Dauphin et al: https://arxiv.org/pdf/1406.2572.pdf (see Equation 1). I don’t see why you would need to re-derive it in your paper and most importantly this should be clearly stated in your paper.\\n\\nPage 2: existence stationary state?\\nWhat are the conditions for the existence of a stationary state? Is bounded noise sufficient?\\n\\nEquation 4 and local approximation\\nRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss-Newton decomposition), the discussion is not precise. The authors claim “Empirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum“\\nBut clearly from the formula itself, one needs to differentiate between minima with low and high function values. For local minima that are high in the energy landscape, the second term might have a large function value. If of course all **depends on the size of the approximation region**. This is extremely important and it is not properly characterized in the paper. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated. Note that similar types of analyses are usually done in dynamical systems where the behavior of a system is linearized around a critical point. In some cases, one can however characterize the size of a basin of attraction, see e.g. the book Nonlinear Systems (3rd Edition): Hassan K. Khalil.\\n\\nAssumption on timescale separation\\nThis section makes numerous claims that are not properly backed up.\\n1) “This assumption is because there is a timescale separation between\\nthe dynamics of \\theta_bar, which relax quickly and the dynamics of \\theta_hat, which evolve much more slowly as the minimal valley is traversed.“\\nAre you claiming the absolute value of the eigenvalue of the non-degenerate space are much smaller than the degenerate space? Why would that be so?\\n2) Ornstein-Uhlenbeck: OU processes require the noise to be Brownian motion. This assumption needs to be clearly stated. Note that there is actually evidence that the noise of SGD is heavy-tail:\\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. \"A tail-index analysis of stochastic gradient noise in deep neural networks.\" arXiv preprint arXiv:1901.06053 (2019).\\n\\nTake away\\nI am unsure what the added value of this paper is. Second-order methods can already be shown to decrease the maximum eigenvalue, see e.g. convergence results derived in Cubic regularization of Newton method and its global performance by Nesterov and Polyak. These methods have also been analyzed in stochastic settings where similar convergence results hold as well. What particular insight do we gain from the results of Theorem 2? The authors claim this could “potentially improve generalization” but this is not justified and no reference is cited.\\n\\n“This indicates that the trace itself is not sufficient to describe generalization (Neyshabur et al., 2017).“\\nI do not see what aspect of Neyshabur et al. justifies your claim, please explain.\\n\\nExperiments\\nAll the experiments performed in the paper are on very small models. Given the rather strong assumptions made in the paper, I feel that the paper should provide stronger empirical evidence to back up their claims.  1571778847695  ICLR.cc/2020/Conference/Paper1047/AnonReviewer1  Official Blind Review #1\n",
      "\n",
      "============================================================\n",
      "SAMPLE REVIEW CONTENT\n",
      "============================================================\n",
      "\n",
      "Paper ID: HkgTTh4FDH\n",
      "Review ID: SkeJiVQ0Fr\n",
      "Rating: 6: Weak Accept\n",
      "\n",
      "Full content:\n",
      "title:\n",
      "Official Blind Review #4\n",
      "\n",
      "rating:\n",
      "6: Weak Accept\n",
      "\n",
      "review:\n",
      "**Contributions:**\n",
      "This paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\n",
      "\n",
      "**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded l2-norm, the classifier learned by GDAT converges to the maximum l2-norm margin classifier at the rate of O(1/√T), exponentially faster than the rate O (1/ log T ) obtained when training with only cleaned data Soudry et al. (2018) [3].\n",
      "\n",
      "**2) theoretical result for Lq norm:** When the adversarial perturbation during training has bounded l_q-norm with q > 1, with a proper choice of c, the gradient descent based adversarial training is directionally convergent to a maximum mixed-norm margin classi...\n"
     ]
    }
   ],
   "source": [
    "# Display the reviews dataframe\n",
    "if len(reviews_df) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"REVIEWS DATAFRAME\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal reviews: {len(reviews_df)}\")\n",
    "    print(f\"\\nReviews per paper:\")\n",
    "    print(reviews_df['paper_id'].value_counts().sort_index())\n",
    "    print(f\"\\n\\nFirst few reviews:\")\n",
    "    print(reviews_df.head(10).to_string())\n",
    "    \n",
    "    # Show sample review content\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE REVIEW CONTENT\")\n",
    "    print(\"=\" * 60)\n",
    "    sample_review = reviews_df.iloc[0]\n",
    "    print(f\"\\nPaper ID: {sample_review['paper_id']}\")\n",
    "    print(f\"Review ID: {sample_review['review_id']}\")\n",
    "    print(f\"Rating: {sample_review['rating']}\")\n",
    "    print(f\"\\nFull content:\")\n",
    "    if sample_review['content']:\n",
    "        print(str(sample_review['content'])[:1000] + (\"...\" if len(str(sample_review['content'])) > 1000 else \"\"))\n",
    "else:\n",
    "    print(\"No reviews to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a14f51fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching reviews...\n",
      "[1/10] Fetching reviews for HkgTTh4FDH...   Found 3 reviews Found 3 reviews\n",
      "[2/10] Fetching reviews for B1erJJrYPH...   Found 3 reviews Found 3 reviews\n",
      "[3/10] Fetching reviews for H1gEP6NFwr...   Found 2 reviews Found 2 reviews\n",
      "[4/10] Fetching reviews for Hklcm0VYDS...   Found 3 reviews Found 3 reviews\n",
      "[5/10] Fetching reviews for BygJKn4tPr...   Found 3 reviews Found 3 reviews\n",
      "[6/10] Fetching reviews for HkerqCEtvS...   Found 4 notes but 0 reviews. Invitation types: {'Desk_Reject': 1, 'Official_Comment': 1, 'Public_Comment': 1, 'Desk_Rejected_Submission': 1} Found 0 reviews\n",
      "[7/10] Fetching reviews for BJgnXpVYwS...   Found 3 reviews Found 3 reviews\n",
      "[8/10] Fetching reviews for rkgqN1SYvr...   Found 3 reviews Found 3 reviews\n",
      "[9/10] Fetching reviews for B1lxV6NFPH...   Found 3 reviews Found 3 reviews\n",
      "[10/10] Fetching reviews for BJlaG0VFDH...   Found 3 reviews Found 3 reviews\n",
      "\n",
      "Total reviews fetched: 26\n",
      "\n",
      "Reviews DataFrame shape: (26, 14)\n",
      "\n",
      "Columns: ['paper_id', 'review_id', 'review_type', 'rating', 'summary', 'strengths', 'weaknesses', 'questions', 'comment', 'review', 'content', 'cdate', 'writer', 'title']\n"
     ]
    }
   ],
   "source": [
    "import openreview\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_value_api2(field):\n",
    "    \"\"\"Extract value from API v2 nested dict format (used by AutoReviewer)\"\"\"\n",
    "    if isinstance(field, dict):\n",
    "        return field.get('value', field)\n",
    "    return field\n",
    "\n",
    "def content_dict_to_text(content_dict):\n",
    "    \"\"\"Extract text from content dictionary\"\"\"\n",
    "    main_keys = [\"title\", \"summary\", \"strengths\", \"weaknesses\", \"questions\", \"comment\", \"metareview\", \"rating\", \"review\"]\n",
    "    text = \"\"\n",
    "    for k in main_keys:\n",
    "        data = content_dict.get(k)\n",
    "        if data is None:\n",
    "            continue\n",
    "        # Handle both old format (direct value) and new format (dict with 'value')\n",
    "        if isinstance(data, dict) and 'value' in data:\n",
    "            text += f\"\\n{k}:\\n{data['value']}\\n\"\n",
    "        else:\n",
    "            text += f\"\\n{k}:\\n{str(data)}\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def get_reviews_for_paper(forum_id, year=2020):\n",
    "    \"\"\"Fetch reviews for a specific paper using OpenReview client (like AutoReviewer)\"\"\"\n",
    "    try:\n",
    "        # Initialize appropriate client based on year\n",
    "        if year < 2024:\n",
    "            client = openreview.Client(baseurl='https://api.openreview.net')\n",
    "        else:\n",
    "            client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')\n",
    "        \n",
    "        # Get all notes in the forum (submission + all replies)\n",
    "        # This is the proper way according to AutoReviewer\n",
    "        notes = client.get_notes(forum=forum_id)\n",
    "        \n",
    "        reviews = []\n",
    "        invitation_types = {}\n",
    "        \n",
    "        for note in notes:\n",
    "            # Get invitation - API v1 uses 'invitation' (string), API v2 uses 'invitations' (list)\n",
    "            if year < 2024:\n",
    "                invitation = getattr(note, 'invitation', '')\n",
    "            else:\n",
    "                invitations = getattr(note, 'invitations', [])\n",
    "                invitation = invitations[0] if invitations else ''\n",
    "            \n",
    "            if invitation:\n",
    "                inv_type = invitation.split(\"/\")[-1]\n",
    "                invitation_types[inv_type] = invitation_types.get(inv_type, 0) + 1\n",
    "            \n",
    "            # Check if it's an Official_Review\n",
    "            if year < 2024:\n",
    "                is_review = 'Official_Review' in invitation\n",
    "            else:\n",
    "                is_review = any('Official_Review' in inv for inv in invitations) if invitations else False\n",
    "            \n",
    "            if is_review:\n",
    "                content = note.content if hasattr(note, 'content') else {}\n",
    "                \n",
    "                review_data = {\n",
    "                    'paper_id': forum_id,\n",
    "                    'review_id': note.id if hasattr(note, 'id') else '',\n",
    "                    'review_type': inv_type if invitation else '',\n",
    "                    'rating': None,\n",
    "                    'summary': None,\n",
    "                    'strengths': None,\n",
    "                    'weaknesses': None,\n",
    "                    'questions': None,\n",
    "                    'comment': None,\n",
    "                    'review': None,  # Full review text\n",
    "                    'content': None,\n",
    "                    'cdate': note.cdate if hasattr(note, 'cdate') else None,\n",
    "                    'writer': note.signatures[0] if hasattr(note, 'signatures') and note.signatures else None,\n",
    "                }\n",
    "                \n",
    "                # Extract fields - handle API v2 nested format\n",
    "                if year < 2024:\n",
    "                    # API v1: direct values\n",
    "                    review_data['rating'] = content.get('rating', '')\n",
    "                    review_data['review'] = content.get('review', '')\n",
    "                    for field in ['summary', 'strengths', 'weaknesses', 'questions', 'comment', 'title']:\n",
    "                        review_data[field] = content.get(field, '')\n",
    "                else:\n",
    "                    # API v2: nested {'value': ...} format\n",
    "                    review_data['rating'] = get_value_api2(content.get('rating', ''))\n",
    "                    review_data['review'] = get_value_api2(content.get('review', ''))\n",
    "                    for field in ['summary', 'strengths', 'weaknesses', 'questions', 'comment', 'title']:\n",
    "                        review_data[field] = get_value_api2(content.get(field, ''))\n",
    "                \n",
    "                # Get full content text for reference\n",
    "                review_data['content'] = content_dict_to_text(content)\n",
    "                \n",
    "                reviews.append(review_data)\n",
    "        \n",
    "        # Debug output\n",
    "        if len(reviews) == 0 and len(notes) > 0:\n",
    "            print(f\"  Found {len(notes)} notes but 0 reviews. Invitation types: {invitation_types}\", end=' ')\n",
    "        else:\n",
    "            print(f\"  Found {len(reviews)} reviews\", end=' ')\n",
    "        \n",
    "        return reviews\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)[:50]}\", end=' ')\n",
    "        return []\n",
    "\n",
    "# Fetch reviews for all papers\n",
    "all_reviews = []\n",
    "print(\"Fetching reviews...\")\n",
    "for i, paper_id in enumerate(paper_ids, 1):\n",
    "    print(f\"[{i}/{len(paper_ids)}] Fetching reviews for {paper_id}...\", end=' ')\n",
    "    try:\n",
    "        reviews = get_reviews_for_paper(paper_id, year=2020)\n",
    "        all_reviews.extend(reviews)\n",
    "        print(f\"Found {len(reviews)} reviews\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:50]}\")\n",
    "    time.sleep(0.5)  # Small delay to avoid rate limiting (like AutoReviewer)\n",
    "\n",
    "print(f\"\\nTotal reviews fetched: {len(all_reviews)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "if all_reviews:\n",
    "    reviews_df = pd.DataFrame(all_reviews)\n",
    "    print(f\"\\nReviews DataFrame shape: {reviews_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(reviews_df.columns)}\")\n",
    "else:\n",
    "    print(\"\\nNo reviews found!\")\n",
    "    reviews_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d75c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REVIEWS DATAFRAME\n",
      "============================================================\n",
      "\n",
      "Total reviews: 26\n",
      "\n",
      "Reviews per paper:\n",
      "paper_id\n",
      "HkgTTh4FDH    3\n",
      "B1erJJrYPH    3\n",
      "Hklcm0VYDS    3\n",
      "BygJKn4tPr    3\n",
      "BJgnXpVYwS    3\n",
      "B1lxV6NFPH    3\n",
      "rkgqN1SYvr    3\n",
      "BJlaG0VFDH    3\n",
      "H1gEP6NFwr    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "First few reviews:\n",
      "     paper_id   review_id      review_type          rating summary strengths weaknesses questions comment                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         content          cdate                                           writer                     title\n",
      "0  HkgTTh4FDH  SkeJiVQ0Fr  Official_Review  6: Weak Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           **Contributions:**\\nThis paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\\n\\n**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded l2-norm, the classifier learned by GDAT converges to the maximum l2-norm margin classifier at the rate of O(1/√T), exponentially faster than the rate O (1/ log T ) obtained when training with only cleaned data Soudry et al. (2018) [3].\\n\\n**2) theoretical result for Lq norm:** When the adversarial perturbation during training has bounded l_q-norm with q > 1, with a proper choice of c, the gradient descent based adversarial training is directionally convergent to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum $l_2$-norm margin classifier under worst-case $l_q$-norm perturbation to the data.\\n\\nThe paper is well written and easy to understand. I haven’t checked the proofs, but I focused on readability and motivations. \\n\\nI have the following questions.\\n\\n**Theoretical questions:**\\n- Can you please explain the intuition of lemma 3.2 and 3.3 for convergence?\\n**Experiments questions:**\\nKeeping in mind that the core of the paper is about proving that GDAT enjoys the same implicit bias as GD and better convergence performance on clean data, I can’t help myself by asking you how these properties reflect in experiments with deep Neural Networks:\\n    - Can you please clarify your motivation for the reduction of the performance gap when the with of the hidden layer increases (Figure 2)? It is not clear what do you mean by ...“as network width increases, the margin on the samples outputted by the hidden layer also increases”...\\nMy personal interpretation of the results is that the inner maximization problem is much harder to solve and being approximated, the effect of the GDAT is much less prominent, meaning that PGD is not enough to benefit from adversarial training in this sense. What do you think about it?\\n    - Take away message of the paper: adversarial training accelerates convergence. Theoretical results are for the training empirical loss.\\nIs this also the case of the validation loss? You observe this behavior for the case of a single MLP on MNIST binary problem 2 vs 9 (appendix E) claiming that adversarial training improves generalization performance.\\nThis is not observed in the literature, it is actually the opposite [1, 2]: it exists a tension between robustness and performance on the clean test set. Why do you think you are not observing this? My guesses:\\n    - you are analysing a very simple binary problem\\n    - your model is very simple and comparable to a linear classifier, so your theoretical results hold.\\n\\n[1][1805.12152 Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)\\n[2][1810.12715 On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)\\n[3][1710.10345 The Implicit Bias of Gradient Descent on Separable Data](https://arxiv.org/abs/1710.10345)\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            title:\\nOfficial Blind Review #4\\n\\nrating:\\n6: Weak Accept\\n\\nreview:\\n**Contributions:**\\nThis paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\\n\\n**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded l2-norm, the classifier learned by GDAT converges to the maximum l2-norm margin classifier at the rate of O(1/√T), exponentially faster than the rate O (1/ log T ) obtained when training with only cleaned data Soudry et al. (2018) [3].\\n\\n**2) theoretical result for Lq norm:** When the adversarial perturbation during training has bounded l_q-norm with q > 1, with a proper choice of c, the gradient descent based adversarial training is directionally convergent to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum $l_2$-norm margin classifier under worst-case $l_q$-norm perturbation to the data.\\n\\nThe paper is well written and easy to understand. I haven’t checked the proofs, but I focused on readability and motivations. \\n\\nI have the following questions.\\n\\n**Theoretical questions:**\\n- Can you please explain the intuition of lemma 3.2 and 3.3 for convergence?\\n**Experiments questions:**\\nKeeping in mind that the core of the paper is about proving that GDAT enjoys the same implicit bias as GD and better convergence performance on clean data, I can’t help myself by asking you how these properties reflect in experiments with deep Neural Networks:\\n    - Can you please clarify your motivation for the reduction of the performance gap when the with of the hidden layer increases (Figure 2)? It is not clear what do you mean by ...“as network width increases, the margin on the samples outputted by the hidden layer also increases”...\\nMy personal interpretation of the results is that the inner maximization problem is much harder to solve and being approximated, the effect of the GDAT is much less prominent, meaning that PGD is not enough to benefit from adversarial training in this sense. What do you think about it?\\n    - Take away message of the paper: adversarial training accelerates convergence. Theoretical results are for the training empirical loss.\\nIs this also the case of the validation loss? You observe this behavior for the case of a single MLP on MNIST binary problem 2 vs 9 (appendix E) claiming that adversarial training improves generalization performance.\\nThis is not observed in the literature, it is actually the opposite [1, 2]: it exists a tension between robustness and performance on the clean test set. Why do you think you are not observing this? My guesses:\\n    - you are analysing a very simple binary problem\\n    - your model is very simple and comparable to a linear classifier, so your theoretical results hold.\\n\\n[1][1805.12152 Robustness May Be at Odds with Accuracy](https://arxiv.org/abs/1805.12152)\\n[2][1810.12715 On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models](https://arxiv.org/abs/1810.12715)\\n[3][1710.10345 The Implicit Bias of Gradient Descent on Separable Data](https://arxiv.org/abs/1710.10345)  1571857559418   ICLR.cc/2020/Conference/Paper244/AnonReviewer4  Official Blind Review #4\n",
      "1  HkgTTh4FDH  H1eIh4uhYB  Official_Review       8: Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. Both l2 and general lq perturbations are studied.\\n\\nFor l2, the authors show that for small enough perturbations, the algorithm converges in direction to the max l2 margin solution, with faster convergence rates compared to standard gradient descent, both for the clean loss and the parameter direction (O(1/sqrt(T)) instead of O(1/log(T))).\\nFor lq, convergence to a different max-margin direction is shown, given according to a mixture of the l2 and lp norms, though the parameter convergence is slower.\\nThese results are further illustrated by numerical experiments.\\n\\nThe topic of the paper is novel and interesting, in particular the finding that adversarial training can lead to benefits in terms of optimization in addition to robustness, as well as the characterization of the inductive bias obtained when using lq perturbations in adversarial training (i.e. a mixture of lp and l2 norms, rather than just lp). Granted, the setting of linear models is a bit limited, but it provides a first step for more realistic models. The paper is also well written, and also has a nice numerical validation of the results. Overall, I am in favor of acceptance.\\n\\nHere are some questions/comments that could be further discussed:\\n* for l2, while the obtained rates for parameter convergence are better in their dependence on T, they depend on a data-dependent quantity alpha in contrast to standard GD, suggesting that the rate could be worse for small alpha: is there a trade-off here? would standard GD be preferable if alpha is small, or could the current rate (7) automatically adapt to such settings?\\n\\n* in Theorem 3.4, how does the choice of c come into play? can you obtain better rates by optimizing it (ideally this should happen when q=2)? Regarding the comment on tending to the lq-norm margin for c -> gamma_q, is this at the cost of poorer convergence?\\n\\n* for lq perturbations, while I agree that the studied algorithm is interesting since that's what's used in practice for deep networks, I do wonder if in this specific setting you could get better convergence (and with the right norm lp instead of the mixture) by optimizing using the appropriate geometry, e.g. with mirror descent.\\n\\n* the analyzed algorithm considers optimal perturbations -- do you have a sense of how robust the theory is to errors in the inner optimization?\\n\\n\\nminor comments/typos:\\n* second bullet in 'main contributions': 'mixed-norm margin' could be further explained, or at least point to the definition. The terminology is also confusing as it sounds like a matrix mixed-norm\\n* 'polyhedral cone ...': bar{u} should be u?\\n* after Theorem 3.2 'not an issue': not so clear, this should be discussed further (see comment above)\\n* section 3.2, \"defer discussion for 1, infty\": you could provide a flavor of the results for these cases in the main text, given their prominence in practice\\n* figure 1(b): for the direction plot, is it actually u_2? how should this plot be interpreted given that the two curves converge to a different direction?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              title:\\nOfficial Blind Review #3\\n\\nrating:\\n8: Accept\\n\\nreview:\\nThe paper studies the implicit bias of gradient based adversarial training for linear models on separable data with the exponential loss. Both l2 and general lq perturbations are studied.\\n\\nFor l2, the authors show that for small enough perturbations, the algorithm converges in direction to the max l2 margin solution, with faster convergence rates compared to standard gradient descent, both for the clean loss and the parameter direction (O(1/sqrt(T)) instead of O(1/log(T))).\\nFor lq, convergence to a different max-margin direction is shown, given according to a mixture of the l2 and lp norms, though the parameter convergence is slower.\\nThese results are further illustrated by numerical experiments.\\n\\nThe topic of the paper is novel and interesting, in particular the finding that adversarial training can lead to benefits in terms of optimization in addition to robustness, as well as the characterization of the inductive bias obtained when using lq perturbations in adversarial training (i.e. a mixture of lp and l2 norms, rather than just lp). Granted, the setting of linear models is a bit limited, but it provides a first step for more realistic models. The paper is also well written, and also has a nice numerical validation of the results. Overall, I am in favor of acceptance.\\n\\nHere are some questions/comments that could be further discussed:\\n* for l2, while the obtained rates for parameter convergence are better in their dependence on T, they depend on a data-dependent quantity alpha in contrast to standard GD, suggesting that the rate could be worse for small alpha: is there a trade-off here? would standard GD be preferable if alpha is small, or could the current rate (7) automatically adapt to such settings?\\n\\n* in Theorem 3.4, how does the choice of c come into play? can you obtain better rates by optimizing it (ideally this should happen when q=2)? Regarding the comment on tending to the lq-norm margin for c -> gamma_q, is this at the cost of poorer convergence?\\n\\n* for lq perturbations, while I agree that the studied algorithm is interesting since that's what's used in practice for deep networks, I do wonder if in this specific setting you could get better convergence (and with the right norm lp instead of the mixture) by optimizing using the appropriate geometry, e.g. with mirror descent.\\n\\n* the analyzed algorithm considers optimal perturbations -- do you have a sense of how robust the theory is to errors in the inner optimization?\\n\\n\\nminor comments/typos:\\n* second bullet in 'main contributions': 'mixed-norm margin' could be further explained, or at least point to the definition. The terminology is also confusing as it sounds like a matrix mixed-norm\\n* 'polyhedral cone ...': bar{u} should be u?\\n* after Theorem 3.2 'not an issue': not so clear, this should be discussed further (see comment above)\\n* section 3.2, \"defer discussion for 1, infty\": you could provide a flavor of the results for these cases in the main text, given their prominence in practice\\n* figure 1(b): for the direction plot, is it actually u_2? how should this plot be interpreted given that the two curves converge to a different direction?  1571746990294   ICLR.cc/2020/Conference/Paper244/AnonReviewer3  Official Blind Review #3\n",
      "2  HkgTTh4FDH  ryePl_e5Kr  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The paper applies theory on implicit bias of gradient descent to an adversarial training toy example. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness. The theoretical results are complemented with experiments on linear classifiers and small neural networks.\\n\\nThe key technical contributions look mostly like an application of the existing theory on implicit bias of gradient descent, thus I would rate the originality of this work as moderate. From the point of view of adversarial machine learning, I don't consider the results to be significant. In particular, there is a large discrepancy between the theoretical results on the toy example and the empirical convergence behaviour of adversarial training of neural networks (compare Figure 2 and 3). In particular, the theoretically derived exponential speed-up of adversarial training for the toy example is not reflected by empirical observations for practically relevant adversarial training tasks (e.g. consider the convergence behaviours reported by Goodfellow et al. (2014) or Madry et al. (2017)). Thus, the theory doesn't seem to be generally applicable beyond the linear toy model. A clear discussion of its limitations is missing.\\n\\nOverall, while this may be an interesting extension of the existing body of literature on the implicit bias of gradient descent, I find that from an adversarial machine learning perspective this paper does not meet its objective to derive generally applicable insights on how adversarial training works.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    title:\\nOfficial Blind Review #1\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThe paper applies theory on implicit bias of gradient descent to an adversarial training toy example. It attempts to utilize the theoretical results for deriving insights on how adversarial training establishes robustness. The theoretical results are complemented with experiments on linear classifiers and small neural networks.\\n\\nThe key technical contributions look mostly like an application of the existing theory on implicit bias of gradient descent, thus I would rate the originality of this work as moderate. From the point of view of adversarial machine learning, I don't consider the results to be significant. In particular, there is a large discrepancy between the theoretical results on the toy example and the empirical convergence behaviour of adversarial training of neural networks (compare Figure 2 and 3). In particular, the theoretically derived exponential speed-up of adversarial training for the toy example is not reflected by empirical observations for practically relevant adversarial training tasks (e.g. consider the convergence behaviours reported by Goodfellow et al. (2014) or Madry et al. (2017)). Thus, the theory doesn't seem to be generally applicable beyond the linear toy model. A clear discussion of its limitations is missing.\\n\\nOverall, while this may be an interesting extension of the existing body of literature on the implicit bias of gradient descent, I find that from an adversarial machine learning perspective this paper does not meet its objective to derive generally applicable insights on how adversarial training works.  1571583982950   ICLR.cc/2020/Conference/Paper244/AnonReviewer1  Official Blind Review #1\n",
      "3  B1erJJrYPH  BJxTPBxM9S  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This paper combines neuron alignment with mode connectivity. Specifically, it applies paths neuron alignment to the calculation of mode-connecting and empirical results show that alignment helps in finding better curves.\\nCombining neuron alignment with mode connectivity seems to be a good idea, but the message the authors want to convey is somewhat vague. Some key details are not presented clearly, and some contents seem to be irrelevant. The following are some detailed comments and questions:\\n1. One main contribution of this paper is the observation that the observation that alignment helps in finding better curves. An observation is excellent if it brings significant performance improvements in practice, or if it brings deep insights in the understanding of the field. However, for the former, the improvement in the performance is not that much; for the latter, there is hardly any insight conveyed by this paper. Therefore, this observation itself is not strong enough.\\n2. One contribution of this paper is applying proximal alternating minimization (PAM) when optimizing the parameters and proving its convergence. Nonetheless, PAM is only used in one model (TinyTen) and does not bring any improvement in the performance. It seems that there is no point in applying PAM and the contents related to PAM are all somewhat irrelevant.\\n3. Usually sufficient details help in good understandings, but in this paper, some key details are unfortunately missing. For example, in Algorithm 2, details on the optimization step is not clear: what is the optimization method the authors use other than PAM? Also, no comments are addressed on Figure 7 to Figure 10, either in the main body or in the appendix. I would like to see more explanations details. If the source code is provided, it will be better.\\nIn sum, the idea seems to be interesting, but the overall quality of the paper is still yet to be improved, and some key details need to be addressed more clearly before it can be accepted as a qualified submission.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           title:\\nOfficial Blind Review #2\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThis paper combines neuron alignment with mode connectivity. Specifically, it applies paths neuron alignment to the calculation of mode-connecting and empirical results show that alignment helps in finding better curves.\\nCombining neuron alignment with mode connectivity seems to be a good idea, but the message the authors want to convey is somewhat vague. Some key details are not presented clearly, and some contents seem to be irrelevant. The following are some detailed comments and questions:\\n1. One main contribution of this paper is the observation that the observation that alignment helps in finding better curves. An observation is excellent if it brings significant performance improvements in practice, or if it brings deep insights in the understanding of the field. However, for the former, the improvement in the performance is not that much; for the latter, there is hardly any insight conveyed by this paper. Therefore, this observation itself is not strong enough.\\n2. One contribution of this paper is applying proximal alternating minimization (PAM) when optimizing the parameters and proving its convergence. Nonetheless, PAM is only used in one model (TinyTen) and does not bring any improvement in the performance. It seems that there is no point in applying PAM and the contents related to PAM are all somewhat irrelevant.\\n3. Usually sufficient details help in good understandings, but in this paper, some key details are unfortunately missing. For example, in Algorithm 2, details on the optimization step is not clear: what is the optimization method the authors use other than PAM? Also, no comments are addressed on Figure 7 to Figure 10, either in the main body or in the appendix. I would like to see more explanations details. If the source code is provided, it will be better.\\nIn sum, the idea seems to be interesting, but the overall quality of the paper is still yet to be improved, and some key details need to be addressed more clearly before it can be accepted as a qualified submission.  1572107621232  ICLR.cc/2020/Conference/Paper1466/AnonReviewer2  Official Blind Review #2\n",
      "4  B1erJJrYPH  SylrU09ptS  Official_Review  6: Weak Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The paper investigates the connection between symmetries in the neural network architectures and the loss landscape of the corresponding neural networks. In the previous works, there was shown that the two local minima of a neural network can be connected by a curve with the low validation/train loss along the curve. Despite the loss on the curve being close to the loss at the end points, there are segments of the curve on which the loss in higher than loss at the local minima. To overcome this problem, the authors proposed two-step procedure: 1. Align weights between two neural networks 2. Launch the path finding algorithm between aligned weights. In other words, the authors proposed to connect not original local minima but local minima that parametrize the same functions as the original ones, but have a different order of neurons. The authors also proposed PAM algorithm where they iteratively apply path finding algorithm and weights alignment algorithm.\\n\\nNovelty and significance. The idea to combine the symmetrization of NN architectures with path finding algorithms is new to the best of my knowledge. Experimentally, the authors showed that ensembling the midpoint and endpoints of the curve found via path finding  algorithm coupled with the neural alignment algorithm delivers a better result than simple averaging of three independent networks. This is a new and significant result, since before the ensembling of points along the curve had the same quality as the ensemble of three independent networks or marginally better.  \\nThe weak side of the paper is the PAM algorithm that occupies a significant part of the paper and does not deliver a significantly better result than the simple application of the neural alignment procedure before launching the path finding algorithm.\\n\\nClarity. Overall, the related work section contains all relevant references to the previous works to the best of my knowledge. The paper is well written, excluding the section Neuron Alignment that lacks notation description.  \\n\\nThe paper contains several typos and inaccuracies:\\n1. “However, We find its empirical performance is similar to standard curve finding, and we advocate the latter for practical use due to computational efficiency.” The word “We” should start with the lowercase letter.\\n2. In the sentence “The first question to address is how to effectively deal with the constraints in equation 6” the index i should be replaced with the index l.\\n3. Notation Π|Kl| introduced after equation 5.\\n4. In the section describing neuron alignment  algorithm Lie et al. [1] used a different notation. So I would recommend to further extend this section and add all necessary notations. Also, I would recommend to add a direct link to the paper where the problem is described in matrix form.\\n5. In the Algorithm 1 “Initialize P θ2 := [Wˆ 2 1 ,Wˆ 2 2 , ...,Wˆ 2 L ] as [W2 1 ,W2 2 , ...,W2 L ] for k ∈ {1, 2, ..., L − 1};”  k is not used anywhere in notation.\\n6. In Figure 3, “model 2 aligned ” sing is out of the plot box for ResNet-32 and VGG-16 architectures.\\n7. The appendix contains the sketch of the proof that is quite difficult to follow. I would recommend giving all necessary definitions as it is done in the [2] and extend the proof. \\n\\nOverall, it is quite an interesting paper but it contains some drawbacks.\\n[1] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E Hopcroft.  Convergent learning: Do different neural networks learn the same representations?  In ICLR, 2016\\n\\n[2]  H́edy Attouch, J́erˆome Bolte, Patrick Redont, and Antoine Soubeyran.  Proximal alternating minimization and projection methods for nonconvex problems:  An approach based on the kurdyka-łojasiewicz inequality.Mathematics of Operations Research, 35(2):438–457, 2010\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         title:\\nOfficial Blind Review #3\\n\\nrating:\\n6: Weak Accept\\n\\nreview:\\nThe paper investigates the connection between symmetries in the neural network architectures and the loss landscape of the corresponding neural networks. In the previous works, there was shown that the two local minima of a neural network can be connected by a curve with the low validation/train loss along the curve. Despite the loss on the curve being close to the loss at the end points, there are segments of the curve on which the loss in higher than loss at the local minima. To overcome this problem, the authors proposed two-step procedure: 1. Align weights between two neural networks 2. Launch the path finding algorithm between aligned weights. In other words, the authors proposed to connect not original local minima but local minima that parametrize the same functions as the original ones, but have a different order of neurons. The authors also proposed PAM algorithm where they iteratively apply path finding algorithm and weights alignment algorithm.\\n\\nNovelty and significance. The idea to combine the symmetrization of NN architectures with path finding algorithms is new to the best of my knowledge. Experimentally, the authors showed that ensembling the midpoint and endpoints of the curve found via path finding  algorithm coupled with the neural alignment algorithm delivers a better result than simple averaging of three independent networks. This is a new and significant result, since before the ensembling of points along the curve had the same quality as the ensemble of three independent networks or marginally better.  \\nThe weak side of the paper is the PAM algorithm that occupies a significant part of the paper and does not deliver a significantly better result than the simple application of the neural alignment procedure before launching the path finding algorithm.\\n\\nClarity. Overall, the related work section contains all relevant references to the previous works to the best of my knowledge. The paper is well written, excluding the section Neuron Alignment that lacks notation description.  \\n\\nThe paper contains several typos and inaccuracies:\\n1. “However, We find its empirical performance is similar to standard curve finding, and we advocate the latter for practical use due to computational efficiency.” The word “We” should start with the lowercase letter.\\n2. In the sentence “The first question to address is how to effectively deal with the constraints in equation 6” the index i should be replaced with the index l.\\n3. Notation Π|Kl| introduced after equation 5.\\n4. In the section describing neuron alignment  algorithm Lie et al. [1] used a different notation. So I would recommend to further extend this section and add all necessary notations. Also, I would recommend to add a direct link to the paper where the problem is described in matrix form.\\n5. In the Algorithm 1 “Initialize P θ2 := [Wˆ 2 1 ,Wˆ 2 2 , ...,Wˆ 2 L ] as [W2 1 ,W2 2 , ...,W2 L ] for k ∈ {1, 2, ..., L − 1};”  k is not used anywhere in notation.\\n6. In Figure 3, “model 2 aligned ” sing is out of the plot box for ResNet-32 and VGG-16 architectures.\\n7. The appendix contains the sketch of the proof that is quite difficult to follow. I would recommend giving all necessary definitions as it is done in the [2] and extend the proof. \\n\\nOverall, it is quite an interesting paper but it contains some drawbacks.\\n[1] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John E Hopcroft.  Convergent learning: Do different neural networks learn the same representations?  In ICLR, 2016\\n\\n[2]  H́edy Attouch, J́erˆome Bolte, Patrick Redont, and Antoine Soubeyran.  Proximal alternating minimization and projection methods for nonconvex problems:  An approach based on the kurdyka-łojasiewicz inequality.Mathematics of Operations Research, 35(2):438–457, 2010  1571823181024  ICLR.cc/2020/Conference/Paper1466/AnonReviewer3  Official Blind Review #3\n",
      "5  B1erJJrYPH  SJeZKdchKH  Official_Review       1: Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Given two parameters theta_1 and theta_2 of the same architecture, the authors propose to learn a minimal loss curve between theta_1 and P theta_2, where P is a permutation matrix yielding another equivalent parameterization of the same network. The authors show that either by initializing P with neuron alignment or by optimizing P jointly with the curve, one can find a parameter-space with better accuracy and loss than by just learning a curve between theta_1 and theta_2. The authors also show that initializing P is sufficient to obtain these gains, avoiding the complexity of also optimizing for P. Furthermore, they show that ensembles across models from these curves have a very mild gain in accuracy to those of non-aligned curves.\\n\\nThe main qualm I have about this paper is about the significance of the contributions and the motivation.\\nAt the core, the authors propose to find a curve between theta_1 and P theta_2 where P comes from aligning theta_1 and theta_2 as in (Li et al. 2016.). This is lacking almost any motivation, or discussion on what problem they are trying to solve. Are they trying to find ensembles with lower error? If that is the case, well the results are evidence of a negative result in this respect, which is okay but given how ad-hoc and poorly motivated the method is to that objective it's not much of a contribution. Are they trying to better understand theoretically the loss landscape of neural networks? I don't think there's any theoretical gain in that regard from this paper either. They show that the curves between aligned networks are better, but they don't show how this relates to anything else in the published literature or open questions in the theoretical deep learning field.\\n\\nRegarding contribution 2, PAM is in the end shown to not converge to better curves than simply initializing with alignment. Also, doesn't the convergence result to a critical point also apply for standard descent methods? The convergence theorem doesn't seem to be much of a contribution in my opinion, either to the optimization or the deep learning community.\\n\\nRegarding contribution 3, I agree that better curves can be learned faster, but why is this a meaningful contribution? What problem that the deep learning or the theoretical machine learning cares about does this help solve?\\n\\nRegarding contribution 4, as the authors themselves admit, the improvement is very dim in comparison to non-aligned curves, and comparisons to other ensemble methods are not present.\\n\\nI want to acknowledge that while the motivation and contributions are in my opinion dim, I find the experimental methodology of this paper very solid. All the claims are to my extent very well validated with experiments, and the experiments are in general well designed to validate those claims. My problem is sadly not with the validity of the claims but with their significance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          title:\\nOfficial Blind Review #1\\n\\nrating:\\n1: Reject\\n\\nreview:\\nGiven two parameters theta_1 and theta_2 of the same architecture, the authors propose to learn a minimal loss curve between theta_1 and P theta_2, where P is a permutation matrix yielding another equivalent parameterization of the same network. The authors show that either by initializing P with neuron alignment or by optimizing P jointly with the curve, one can find a parameter-space with better accuracy and loss than by just learning a curve between theta_1 and theta_2. The authors also show that initializing P is sufficient to obtain these gains, avoiding the complexity of also optimizing for P. Furthermore, they show that ensembles across models from these curves have a very mild gain in accuracy to those of non-aligned curves.\\n\\nThe main qualm I have about this paper is about the significance of the contributions and the motivation.\\nAt the core, the authors propose to find a curve between theta_1 and P theta_2 where P comes from aligning theta_1 and theta_2 as in (Li et al. 2016.). This is lacking almost any motivation, or discussion on what problem they are trying to solve. Are they trying to find ensembles with lower error? If that is the case, well the results are evidence of a negative result in this respect, which is okay but given how ad-hoc and poorly motivated the method is to that objective it's not much of a contribution. Are they trying to better understand theoretically the loss landscape of neural networks? I don't think there's any theoretical gain in that regard from this paper either. They show that the curves between aligned networks are better, but they don't show how this relates to anything else in the published literature or open questions in the theoretical deep learning field.\\n\\nRegarding contribution 2, PAM is in the end shown to not converge to better curves than simply initializing with alignment. Also, doesn't the convergence result to a critical point also apply for standard descent methods? The convergence theorem doesn't seem to be much of a contribution in my opinion, either to the optimization or the deep learning community.\\n\\nRegarding contribution 3, I agree that better curves can be learned faster, but why is this a meaningful contribution? What problem that the deep learning or the theoretical machine learning cares about does this help solve?\\n\\nRegarding contribution 4, as the authors themselves admit, the improvement is very dim in comparison to non-aligned curves, and comparisons to other ensemble methods are not present.\\n\\nI want to acknowledge that while the motivation and contributions are in my opinion dim, I find the experimental methodology of this paper very solid. All the claims are to my extent very well validated with experiments, and the experiments are in general well designed to validate those claims. My problem is sadly not with the validity of the claims but with their significance.  1571756153378  ICLR.cc/2020/Conference/Paper1466/AnonReviewer1  Official Blind Review #1\n",
      "6  H1gEP6NFwr  BylSI7T6qr  Official_Review  3: Weak Reject                                                 This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. The tunability of the optimizer is a weighted sum of best performance at a given budget. The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer’s learning rate is likely to be a very good choice; it doesn’t guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations.\\n\\nComments:\\n\\nThe paper is easy to follow. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons:\\n\\nIn section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of “sharpness” of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions. Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails.\\n\\nIn addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. \\n\\nThe definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable. \\n\\nThe authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO. My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. \\n\\nMy major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM. Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.\\n\\n\\nThe author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained. Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?\\n\\n[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451\\n\\n[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489\\n\\n[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350\\n\\n[4] Rethinking the Hyperparameters for Fine-tuning\\n https://openreview.net/forum?id=B1g8VkHFPH\\n  title:\\nOfficial Blind Review #2\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThis paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints. The tunability of the optimizer is a weighted sum of best performance at a given budget. The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer’s learning rate is likely to be a very good choice; it doesn’t guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations.\\n\\nComments:\\n\\nThe paper is easy to follow. The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons:\\n\\nIn section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of “sharpness” of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions. Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails.\\n\\nIn addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets. \\n\\nThe definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets? The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable. \\n\\nThe authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO. My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. \\n\\nMy major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge. A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost. My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM. Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.\\n\\n\\nThe author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained. Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?\\n\\n[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451\\n\\n[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489\\n\\n[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350\\n\\n[4] Rethinking the Hyperparameters for Fine-tuning\\n https://openreview.net/forum?id=B1g8VkHFPH  1572881228757   ICLR.cc/2020/Conference/Paper593/AnonReviewer2  Official Blind Review #2\n",
      "7  H1gEP6NFwr  SJx0zByAYr  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The main contributions of the submission are:\\n\\n1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).\\n2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer. This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned. The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are \"hyper-parameters\" of the metric itself. They use K=100 and suggest 3 possible choices of weights.\\n\\nThe paper appears to treat 2. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset. Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics. Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).\\n\\nA stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past. Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work. Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.\\n\\nThey also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.\\n\\nI enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.\\n\\nOther comments/notes:\\n* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials. I think it would be worth discussing this more.\\n* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)\\n* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    title:\\nOfficial Blind Review #1\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThe main contributions of the submission are:\\n\\n1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).\\n2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer. This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned. The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K. The weights of this weighted average and K are \"hyper-parameters\" of the metric itself. They use K=100 and suggest 3 possible choices of weights.\\n\\nThe paper appears to treat 2. as the main contribution. However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters). The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset. Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics. Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).\\n\\nA stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past. Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work. Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.\\n\\nThey also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.\\n\\nI enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.\\n\\nOther comments/notes:\\n* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials. I think it would be worth discussing this more.\\n* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)\\n* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases.  1571841301558   ICLR.cc/2020/Conference/Paper593/AnonReviewer1  Official Blind Review #1\n",
      "8  Hklcm0VYDS  BJxOztS89r  Official_Review  6: Weak Accept                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      This paper studies the behavior of SGD after it has reached a steady state and settled in a minimal valley. The authors show that even if SGD has reached a minimal valley, the noisy updates provided by a minibatch of samples result in the trace of the Hessians reducing with further SGD iterations. The authors also show that by changing the type of noise that is added to GD, one can get different functions of the Hessian to reduce with SGD iterations. The theoretical conclusions are then verified empirically with synthetic examples and real deep learning examples\\n\\nIn my opinion this is an interesting paper and research direction that helps us understand how SGD biases solutions towards \"flatter\" minima as measured by the trace norm of the Hessian of the loss function. I have a few concerns though:\\n\\n1. Is the steady state assumption valid for neural network training? Especially when training on exponential losses (like cross entropy) which drives the parameters towards large norm solutions? This seems to be an important yet unsupported assumption in the analysis.\\n\\n2. Does the Hessian-noise covariance alignment exist for squared loss functions as well? Is this specific to log-likelihood models?\\n\\n3. In the experiments to delineate how different types of noise can result in regularization of different quantities, is there a reason only a synthetic example is used? Can this be replicated for deep networks, or are the two quantities (trace vs determinant) closely related? It would be nice to see how this behavior extends to deep networks.\\n\\nAdditional questions/comments:\\n1. While overparameterization seems to be important to the analysis (the parameters move in the degenerate subspace but not in the non-degenerate one), is there anyway one can amend this framework to analyze different levels of overparameterization? Is there any difference in the rates of decrease in the trace norms for highly overparameterized models vs lightly overparameterized ones?\\n\\n2. This paper talks about how SGD has a bias towards flatter minima. Is there a reason to prefer flatter solutions over sharper ones to get better generalization? Can one prove this connection?\\n\\n3. In the Densenet experiments in Figure 4a, it seems as though this relationship between flatness and generalization might not hold? There are points along the optimization trajectory where the validation loss seems to be better than at the last few points along the optimization path. However at the former set of points the trace norm of the hessian is larger than at the latter points. Is this from a stray experiment, or are the plots averaged over a number of different runs? Please add these details as well as other details such as learning rates, criteria to decide when steady state was reached, whether or not batch norm was used, etc.\\n\\n4. The Fluctuation-Dissipation Relations need to be explained more clearly. For people unfamiliar with statistical physics (and Yaida 2019) the notation and formulation is not immediately clear and that makes the paper harder to read.\\n\\nOverall I believe this paper explains an important phenomenon. I am willing to update my score if my concerns are addressed/if there is any misunderstanding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     title:\\nOfficial Blind Review #2\\n\\nrating:\\n6: Weak Accept\\n\\nreview:\\nThis paper studies the behavior of SGD after it has reached a steady state and settled in a minimal valley. The authors show that even if SGD has reached a minimal valley, the noisy updates provided by a minibatch of samples result in the trace of the Hessians reducing with further SGD iterations. The authors also show that by changing the type of noise that is added to GD, one can get different functions of the Hessian to reduce with SGD iterations. The theoretical conclusions are then verified empirically with synthetic examples and real deep learning examples\\n\\nIn my opinion this is an interesting paper and research direction that helps us understand how SGD biases solutions towards \"flatter\" minima as measured by the trace norm of the Hessian of the loss function. I have a few concerns though:\\n\\n1. Is the steady state assumption valid for neural network training? Especially when training on exponential losses (like cross entropy) which drives the parameters towards large norm solutions? This seems to be an important yet unsupported assumption in the analysis.\\n\\n2. Does the Hessian-noise covariance alignment exist for squared loss functions as well? Is this specific to log-likelihood models?\\n\\n3. In the experiments to delineate how different types of noise can result in regularization of different quantities, is there a reason only a synthetic example is used? Can this be replicated for deep networks, or are the two quantities (trace vs determinant) closely related? It would be nice to see how this behavior extends to deep networks.\\n\\nAdditional questions/comments:\\n1. While overparameterization seems to be important to the analysis (the parameters move in the degenerate subspace but not in the non-degenerate one), is there anyway one can amend this framework to analyze different levels of overparameterization? Is there any difference in the rates of decrease in the trace norms for highly overparameterized models vs lightly overparameterized ones?\\n\\n2. This paper talks about how SGD has a bias towards flatter minima. Is there a reason to prefer flatter solutions over sharper ones to get better generalization? Can one prove this connection?\\n\\n3. In the Densenet experiments in Figure 4a, it seems as though this relationship between flatness and generalization might not hold? There are points along the optimization trajectory where the validation loss seems to be better than at the last few points along the optimization path. However at the former set of points the trace norm of the hessian is larger than at the latter points. Is this from a stray experiment, or are the plots averaged over a number of different runs? Please add these details as well as other details such as learning rates, criteria to decide when steady state was reached, whether or not batch norm was used, etc.\\n\\n4. The Fluctuation-Dissipation Relations need to be explained more clearly. For people unfamiliar with statistical physics (and Yaida 2019) the notation and formulation is not immediately clear and that makes the paper harder to read.\\n\\nOverall I believe this paper explains an important phenomenon. I am willing to update my score if my concerns are addressed/if there is any misunderstanding.  1572391183537  ICLR.cc/2020/Conference/Paper1047/AnonReviewer2  Official Blind Review #2\n",
      "9  Hklcm0VYDS  H1lum-xpYr  Official_Review  3: Weak Reject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This paper proves that under certain conditions, SGD on average decreases the\\ntrace of the Hessian of the loss. The paper is overall clear and the topic addressed in the paper is relevant in machine learning but I don’t think this paper is ready for publication. There are numerous assumptions that are made in the paper, but not properly stated or backed up. In my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.\\n\\nLemma 1\\nThis is a corollary of Morse lemma, which is also used in Dauphin et al: https://arxiv.org/pdf/1406.2572.pdf (see Equation 1). I don’t see why you would need to re-derive it in your paper and most importantly this should be clearly stated in your paper.\\n\\nPage 2: existence stationary state?\\nWhat are the conditions for the existence of a stationary state? Is bounded noise sufficient?\\n\\nEquation 4 and local approximation\\nRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss-Newton decomposition), the discussion is not precise. The authors claim “Empirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum“\\nBut clearly from the formula itself, one needs to differentiate between minima with low and high function values. For local minima that are high in the energy landscape, the second term might have a large function value. If of course all **depends on the size of the approximation region**. This is extremely important and it is not properly characterized in the paper. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated. Note that similar types of analyses are usually done in dynamical systems where the behavior of a system is linearized around a critical point. In some cases, one can however characterize the size of a basin of attraction, see e.g. the book Nonlinear Systems (3rd Edition): Hassan K. Khalil.\\n\\nAssumption on timescale separation\\nThis section makes numerous claims that are not properly backed up.\\n1) “This assumption is because there is a timescale separation between\\nthe dynamics of \\theta_bar, which relax quickly and the dynamics of \\theta_hat, which evolve much more slowly as the minimal valley is traversed.“\\nAre you claiming the absolute value of the eigenvalue of the non-degenerate space are much smaller than the degenerate space? Why would that be so?\\n2) Ornstein-Uhlenbeck: OU processes require the noise to be Brownian motion. This assumption needs to be clearly stated. Note that there is actually evidence that the noise of SGD is heavy-tail:\\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. \"A tail-index analysis of stochastic gradient noise in deep neural networks.\" arXiv preprint arXiv:1901.06053 (2019).\\n\\nTake away\\nI am unsure what the added value of this paper is. Second-order methods can already be shown to decrease the maximum eigenvalue, see e.g. convergence results derived in Cubic regularization of Newton method and its global performance by Nesterov and Polyak. These methods have also been analyzed in stochastic settings where similar convergence results hold as well. What particular insight do we gain from the results of Theorem 2? The authors claim this could “potentially improve generalization” but this is not justified and no reference is cited.\\n\\n“This indicates that the trace itself is not sufficient to describe generalization (Neyshabur et al., 2017).“\\nI do not see what aspect of Neyshabur et al. justifies your claim, please explain.\\n\\nExperiments\\nAll the experiments performed in the paper are on very small models. Given the rather strong assumptions made in the paper, I feel that the paper should provide stronger empirical evidence to back up their claims.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              title:\\nOfficial Blind Review #1\\n\\nrating:\\n3: Weak Reject\\n\\nreview:\\nThis paper proves that under certain conditions, SGD on average decreases the\\ntrace of the Hessian of the loss. The paper is overall clear and the topic addressed in the paper is relevant in machine learning but I don’t think this paper is ready for publication. There are numerous assumptions that are made in the paper, but not properly stated or backed up. In my view, the experimental results are also not sufficient to compensate for the shortcomings of the theoretical part.\\n\\nLemma 1\\nThis is a corollary of Morse lemma, which is also used in Dauphin et al: https://arxiv.org/pdf/1406.2572.pdf (see Equation 1). I don’t see why you would need to re-derive it in your paper and most importantly this should be clearly stated in your paper.\\n\\nPage 2: existence stationary state?\\nWhat are the conditions for the existence of a stationary state? Is bounded noise sufficient?\\n\\nEquation 4 and local approximation\\nRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss-Newton decomposition), the discussion is not precise. The authors claim “Empirically, negative eigenvalues are exponentially small in magnitude compared to major positive ones when a model is close to a minimum“\\nBut clearly from the formula itself, one needs to differentiate between minima with low and high function values. For local minima that are high in the energy landscape, the second term might have a large function value. If of course all **depends on the size of the approximation region**. This is extremely important and it is not properly characterized in the paper. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated. Note that similar types of analyses are usually done in dynamical systems where the behavior of a system is linearized around a critical point. In some cases, one can however characterize the size of a basin of attraction, see e.g. the book Nonlinear Systems (3rd Edition): Hassan K. Khalil.\\n\\nAssumption on timescale separation\\nThis section makes numerous claims that are not properly backed up.\\n1) “This assumption is because there is a timescale separation between\\nthe dynamics of \\theta_bar, which relax quickly and the dynamics of \\theta_hat, which evolve much more slowly as the minimal valley is traversed.“\\nAre you claiming the absolute value of the eigenvalue of the non-degenerate space are much smaller than the degenerate space? Why would that be so?\\n2) Ornstein-Uhlenbeck: OU processes require the noise to be Brownian motion. This assumption needs to be clearly stated. Note that there is actually evidence that the noise of SGD is heavy-tail:\\nSimsekli, Umut, Levent Sagun, and Mert Gurbuzbalaban. \"A tail-index analysis of stochastic gradient noise in deep neural networks.\" arXiv preprint arXiv:1901.06053 (2019).\\n\\nTake away\\nI am unsure what the added value of this paper is. Second-order methods can already be shown to decrease the maximum eigenvalue, see e.g. convergence results derived in Cubic regularization of Newton method and its global performance by Nesterov and Polyak. These methods have also been analyzed in stochastic settings where similar convergence results hold as well. What particular insight do we gain from the results of Theorem 2? The authors claim this could “potentially improve generalization” but this is not justified and no reference is cited.\\n\\n“This indicates that the trace itself is not sufficient to describe generalization (Neyshabur et al., 2017).“\\nI do not see what aspect of Neyshabur et al. justifies your claim, please explain.\\n\\nExperiments\\nAll the experiments performed in the paper are on very small models. Given the rather strong assumptions made in the paper, I feel that the paper should provide stronger empirical evidence to back up their claims.  1571778847695  ICLR.cc/2020/Conference/Paper1047/AnonReviewer1  Official Blind Review #1\n",
      "\n",
      "============================================================\n",
      "SAMPLE REVIEW CONTENT\n",
      "============================================================\n",
      "\n",
      "Paper ID: HkgTTh4FDH\n",
      "Review ID: SkeJiVQ0Fr\n",
      "Rating: 6: Weak Accept\n",
      "\n",
      "Content preview:\n",
      "title:\n",
      "Official Blind Review #4\n",
      "\n",
      "rating:\n",
      "6: Weak Accept\n",
      "\n",
      "review:\n",
      "**Contributions:**\n",
      "This paper extends results on the implicit bias of gradient descent (GD) Soudry et al. (2018)[3] for the case of gradient descent based adversarial training (GDAT).\n",
      "\n",
      "**1) Theoretical result for L2 norm:** Convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD: For any fixed iteration T, when the adversarial perturbation during training has bounded...\n"
     ]
    }
   ],
   "source": [
    "# Display the reviews dataframe\n",
    "if len(reviews_df) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"REVIEWS DATAFRAME\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal reviews: {len(reviews_df)}\")\n",
    "    print(f\"\\nReviews per paper:\")\n",
    "    print(reviews_df['paper_id'].value_counts())\n",
    "    print(f\"\\n\\nFirst few reviews:\")\n",
    "    print(reviews_df.head(10).to_string())\n",
    "    \n",
    "    # Show sample review content\n",
    "    if len(reviews_df) > 0:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SAMPLE REVIEW CONTENT\")\n",
    "        print(\"=\" * 60)\n",
    "        sample_review = reviews_df.iloc[0]\n",
    "        print(f\"\\nPaper ID: {sample_review['paper_id']}\")\n",
    "        print(f\"Review ID: {sample_review['review_id']}\")\n",
    "        print(f\"Rating: {sample_review['rating']}\")\n",
    "        print(f\"\\nContent preview:\")\n",
    "        if sample_review['content']:\n",
    "            print(str(sample_review['content'])[:500] + \"...\")\n",
    "else:\n",
    "    print(\"No reviews to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/skonan/iclr-reviews-2020-2026\n",
    "\n",
    "# TODO a join of these with the parquet file of iclr-datasets to see common paper ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iclr-dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
