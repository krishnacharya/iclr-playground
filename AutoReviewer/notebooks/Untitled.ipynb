{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74947a75-73c0-4f1d-80c9-bfec6c1f9b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/fs/vision-mix/sk7524/NipsIclrData/AutoReviewer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc54379-5b6f-4d51-aace-99fb70fff429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/test_normalized_reviews/preview.parquet')\n",
    "bad = df.loc[df.normalized_json.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ea135fb-7c85-4701-959d-154d1fa9531d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "submission_id                                                  ryxz8CVYDH\n",
       "year                                                                 2020\n",
       "md_path                 data/full_run/normalized/2020/ryxz8CVYDH/ryxz8...\n",
       "type                                                               review\n",
       "review_index                                                            0\n",
       "original_json           {\\n  \"rating\": \"6: Weak Accept\",\\n  \"review\": ...\n",
       "prompt                  <|im_start|>user\\nExtract and normalize this I...\n",
       "generated_texts         [{\\n  \"summary\": \"The paper introduces a zerot...\n",
       "normalized_json         {\"summary\":\"The paper introduces a zeroth-orde...\n",
       "parse_success                                                        True\n",
       "num_generated_tokens                                                  707\n",
       "time_taken_llm                                                       None\n",
       "metrics                                                              None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.iloc[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e67f1cf-2c37-49a4-85e9-bc972a79aea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rating\": \"6: Weak Accept\",\n",
      "  \"review\": \"The paper proposes a zeroth-order optimization framework that employs an RNN to modulate the sampling used to estimate gradients and a second RNN that models the parameter update. More specifically, query directions are sampled from a Gaussian distribution with a diagonal covariance whose evolution is determined by an RNN (QueryRNN). The resulting gradient estimates are used by an RNN (UpdateRNN) that learns the parameter update rule. This framework has the stated advantage that, unlike existing work in ZO optimization, it does not rely upon hand-designed strategies to reduce the variance common to ZO gradient estimators. The paper evaluates the proposed framework on MNIST and CFAR tasks, as well as a synthetic binary classification task. The results demonstrate faster convergence compared to baseline zeroth-order optimization algorithms, while ablations indicate the contributions of the different model components.\\n\\nThe primary contribution of the proposed method is the inclusion of a second network that learns to adapt the covariance of the Gaussian from which  query directions are sampled. The use of an RNN to model the parameter update rule is borrowed from previous work. The ablation studies show that the adaptive sampling strategy noticeably improves convergence as does the inclusion of the RNN update (the contribution of UpdateRNN is more significant in one ablation study, while the contribution of QueryRNN is greater in the other). \\n\\nGiven that a stated advantage of QueryRNN is reducing variance, it would be beneficial to compare against baselines such as Liu et al., 2018a,b and/or Guo et al., 2016 which similarly seek to reduce variance.\\n\\nThe paper includes a large number of typos (e.g., CIFAR-->CIAFR) and grammatical errors, but is otherwise clear.\\n\\nADDITIONAL COMMENTS/QUESTIONS\\n\\n* The related work discussion would benefit from a discussion of how Liu et al. 2018 and Guo et al. 2016 reduce variance\\n\\n* The computational complexity of the proposed method as compared to the baselines is unclear as is the scalability with dimensionality. At various points, the paper comments that other methods scale poorly with the dimensionality of the query space, which is true of the proposed method unless the operations are parallelized. Is this not possible with the baseline methods?\\n\\n* The paper makes hand wavy claims to the fact that modulating the diagonal covariance matrix allows the method to focus on certain subspaces. It would be helpful to make these claims more formal, particularly in light of the fact that the mean does not change.\\n\\n* The method relies upon a bit of a hack that samples from a standard Gaussian at random times. How important is this to performance? How sensitive is convergence to the frequency with which standard Gaussian sampling is used?\\n\\n* The discussion of Eqn. 5 as it relates to Eqn. 1 is unclear.\\n\\n\\nUPDATE AFTER AUTHOR RESPONSE\\n\\nI appreciate the authors' thorough response, which resolved my primary questions/concerns, including comparisons to existing variance reduction methods (which should be incorporated into the main paper).\",\n",
      "  \"title\": \"Official Blind Review #3\",\n",
      "  \"experience_assessment\": \"I do not know much about this area.\",\n",
      "  \"review_assessment_derivations\": \"I assessed the sensibility of the derivations and theory.\",\n",
      "  \"review_assessment_experiments\": \"I carefully checked the experiments.\",\n",
      "  \"review_assessment_reading\": \"I read the paper thoroughly.\",\n",
      "  \"number_of_author_responses\": 2,\n",
      "  \"number_of_reviewer_responses_to_author\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(row.original_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e989b87-4f98-4efd-a97c-5888841db605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": \"The paper introduces a zeroth-order optimization framework that uses two RNNs: one (QueryRNN) to adaptively modulate the diagonal covariance of a Gaussian distribution for sampling query directions, and another (UpdateRNN) to learn the parameter update rule. The method aims to reduce variance in gradient estimation without relying on hand-designed strategies. Experiments on MNIST, CIFAR, and a synthetic binary classification task show faster convergence compared to baselines. Ablation studies confirm the contributions of both components, with QueryRNN improving sampling adaptivity and UpdateRNN enhancing update learning. The authors' response addressed key concerns, particularly regarding variance reduction comparisons.\",\n",
      "  \"strengths\": \"The proposed framework demonstrates faster convergence than baseline zeroth-order methods. The ablation studies effectively validate the contributions of both the QueryRNN and UpdateRNN components. The method's ability to adaptively modulate sampling covariance without hand-designed variance reduction strategies is a notable advantage. The authors' response was thorough and resolved major concerns.\",\n",
      "  \"weaknesses\": \"The paper contains numerous typos and grammatical errors (e.g., 'CIFAR' miswritten as 'CIAFR'). The computational complexity and scalability of the method with respect to dimensionality are unclear, especially given the reliance on RNNs. The claim that modulating the diagonal covariance allows focus on certain subspaces lacks formal justification, particularly since the mean remains unchanged. The reliance on random sampling from a standard Gaussian at intervals is described as a 'hack' and its impact on performance is not well understood. The discussion of Equation 5 in relation to Equation 1 is unclear.\",\n",
      "  \"questions\": [\n",
      "    \"How important is the random sampling from a standard Gaussian to the method's performance, and how sensitive is convergence to the frequency of this sampling?\",\n",
      "    \"How does the proposed method's computational complexity compare to baselines, and is parallelization feasible for both the proposed method and baselines?\",\n",
      "    \"Can the claim that modulating the diagonal covariance enables focus on certain subspaces be formalized, especially given that the mean does not change?\",\n",
      "    \"How does the method relate to Equation 5 in the context of Equation 1?\",\n",
      "    \"Why is the related work discussion missing a comparison of how Liu et al. 2018 and Guo et al. 2016 reduce variance?\"\n",
      "  ],\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"ref\": \"Liu et al., 2018a,b\",\n",
      "      \"within\": \"weaknesses\"\n",
      "    },\n",
      "    {\n",
      "      \"ref\": \"Guo et al., 2016\",\n",
      "      \"within\": \"weaknesses\"\n",
      "    }\n",
      "  ],\n",
      "  \"technical_novelty_contribution\": \"The primary technical novelty lies in using an RNN (QueryRNN) to adaptively learn the diagonal covariance of the Gaussian query distribution, enabling automatic variance reduction without hand-designed strategies. The use of UpdateRNN to learn the update rule is borrowed from prior work.\",\n",
      "  \"empirical_novelty_contribution\": \"The empirical contribution is demonstrated through faster convergence on benchmark tasks (MNIST, CIFAR, synthetic) compared to existing ZO methods, with ablation studies supporting the effectiveness of both components.\",\n",
      "  \"presentation\": \"The paper is otherwise clear in its technical exposition but suffers from a significant number of typos and grammatical errors, which detract from its overall quality.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(row.generated_texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
