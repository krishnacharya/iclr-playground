{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis Results\n",
    "\n",
    "This notebook visualizes the results from TF-IDF baseline experiments.\n",
    "\n",
    "**Configurations tested:**\n",
    "- Text types: `original` (raw MinerU markdown) vs `clean` (normalized markdown)\n",
    "- Targets:\n",
    "  - `binary`: accept (1) vs reject (0)\n",
    "  - `decision`: 4-class (reject, poster, spotlight, oral)\n",
    "  - `citation`: citations_normalized_by_year (percentile rank within year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_base = Path(\"generated_tf_idf_analysis\")\n",
    "\n",
    "configs = [\n",
    "    (\"original\", \"binary\"),\n",
    "    (\"original\", \"decision\"),\n",
    "    (\"original\", \"citation\"),\n",
    "    (\"clean\", \"binary\"),\n",
    "    (\"clean\", \"decision\"),\n",
    "    (\"clean\", \"citation\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for text_type, target in configs:\n",
    "    results_path = output_base / text_type / target / \"test_results.csv\"\n",
    "    if results_path.exists():\n",
    "        df = pd.read_csv(results_path)\n",
    "        all_results.append(df)\n",
    "        print(f\"Loaded: {text_type}/{target}\")\n",
    "    else:\n",
    "        print(f\"Missing: {text_type}/{target}\")\n",
    "\n",
    "if all_results:\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\nTotal experiments: {len(combined)}\")\n",
    "else:\n",
    "    print(\"No results found. Run sbatch/tf_idf_grid_search.sbatch first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    display(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Results (Binary & Decision)\n",
    "\n",
    "For binary and decision targets, we report precision, recall, F1 using binary mapping:\n",
    "- accept (poster, spotlight, oral) = 1\n",
    "- reject = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    classification_results = combined[combined['target'].isin(['binary', 'decision'])]\n",
    "    if len(classification_results) > 0:\n",
    "        print(\"Classification Results (accept vs reject metrics):\")\n",
    "        print(\"=\"*60)\n",
    "        display(classification_results[['text_type', 'target', 'accuracy', 'precision', 'recall', 'f1', 'train_size', 'test_size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regression Results (Citation)\n",
    "\n",
    "For citation prediction, we report Pearson correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    regression_results = combined[combined['target'] == 'citation']\n",
    "    if len(regression_results) > 0:\n",
    "        print(\"Citation Prediction Results:\")\n",
    "        print(\"=\"*60)\n",
    "        display(regression_results[['text_type', 'target', 'correlation', 'p_value', 'train_size', 'test_size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top Features for Binary Classification (Accept vs Reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_type in ['original', 'clean']:\n",
    "    features_path = output_base / text_type / \"binary\" / \"generated_features.csv\"\n",
    "    if features_path.exists():\n",
    "        df = pd.read_csv(features_path)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Top Features - {text_type.upper()} text (Binary)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nTop 20 ACCEPT features:\")\n",
    "        print(df['accept'].tolist())\n",
    "        print(f\"\\nTop 20 REJECT features:\")\n",
    "        print(df['reject'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top Features for Decision Classification (4-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_type in ['original', 'clean']:\n",
    "    features_path = output_base / text_type / \"decision\" / \"generated_features.csv\"\n",
    "    if features_path.exists():\n",
    "        df = pd.read_csv(features_path)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Top Features - {text_type.upper()} text (Decision)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for col in df.columns:\n",
    "            print(f\"\\nTop 20 {col.upper()} features:\")\n",
    "            print(df[col].dropna().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top Features by Citation Quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_type in ['original', 'clean']:\n",
    "    features_path = output_base / text_type / \"citation\" / \"generated_features.csv\"\n",
    "    if features_path.exists():\n",
    "        df = pd.read_csv(features_path)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Top Features by Citation Quartile - {text_type.upper()} text\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for col in df.columns:\n",
    "            print(f\"\\nQuartile {col} (top 15):\")\n",
    "            print(df[col].dropna().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: Original vs Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results and len(combined) > 0:\n",
    "    # Pivot for comparison\n",
    "    comparison = combined.pivot(index='target', columns='text_type', values='f1')\n",
    "    if 'f1' in combined.columns:\n",
    "        print(\"F1 Score Comparison: Original vs Clean\")\n",
    "        print(\"=\"*40)\n",
    "        display(comparison)\n",
    "        \n",
    "        # Bar chart\n",
    "        if len(comparison.dropna()) > 0:\n",
    "            comparison.plot(kind='bar', figsize=(8, 5))\n",
    "            plt.title('F1 Score: Original vs Clean Text')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.xlabel('Target')\n",
    "            plt.legend(title='Text Type')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
