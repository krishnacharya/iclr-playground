[project]
name = "ml-cuda12-env"
version = "0.1.0"
description = "ML environment with CUDA 12.8 support"
requires-python = "==3.12"

dependencies = [
    "vllm==0.11.1",
    "transformers==4.57.1",
    # Torch stack (via CUDA 12.8 index below)
    "torch==2.9.0",
    "torchvision",
    "torchaudio",
    # Ray with LLM extras
    "ray[llm]==2.52.0",
    "accelerate==1.12.0",
    "datasets==4.4.1",
    "numpy==2.2.6",
    "pandas==2.3.3",
    "scikit-learn==1.7.2",
    "tqdm==4.67.1",
    "requests",
    "fastapi",
    "uvicorn==0.38.0",
    "pydantic",
    "blobfile==3.1.0",
    "setuptools>=77.0.3,<80",
    "flash-attn==2.8.1",
    "matplotlib==3.10.6",
    "peft==0.18.0",
    "backoff==2.2.1",
    "bitsandbytes==0.48.2",
    "python-dotenv==1.2.1",
    "hf-transfer==0.1.9",
    "ratelimit==2.2.1",
    "seaborn==0.13.2",
    "pyarrow==22.0.0",
    "nltk==3.9.2",
    "openreview-py==1.52.6",
    "plotly==6.3.1",
    "pyyaml==6.0.3",
    "qps-limit==1.2.7",
    "scipy==1.15.3",
    "textstat==0.7.10",
    "wordcloud==1.9.4",
    "uvloop==0.21.0",
    "chromadb==1.3.5",
    "google-generativeai>=0.8.5",
    "google-genai>=1.52.0",
    "wandb==0.23.0", # marker-pdf
    "mineru[core,vllm]==2.6.2",
    "pip>=25.3",
    "uvloop==0.21.0",
    "simplejson",
    "multiprocess==0.70.11",
    "pymupdf"
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["lib"]

# ------------------------------
# uv configuration
# ------------------------------

[tool.uv]
index-url = "https://pypi.org/simple"
index-strategy = "unsafe-best-match"

# Torch CUDA 12.8 wheels
[[tool.uv.index]]
name = "torch-cu128"
url = "https://download.pytorch.org/whl/cu128"

[tool.uv.sources]
torch = { index = "torch-cu128" }
torchvision = { index = "torch-cu128" }
torchaudio = { index = "torch-cu128" }
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.1/flash_attn-2.8.1+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl" }

# Extra build hooks for flash-attn
[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true }]

# Skip CUDA build for flash-attn (use cached wheel)
[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }

[dependency-groups]
dev = [
    "ipykernel==6.30.1",
]
